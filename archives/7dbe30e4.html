<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico"><link rel="mask-icon" href="/images/favicon.ico" color="#222"><meta name="google-site-verification" content="35AYyqm-wpmGmXtkn-vQMrk7AkFl1Do55uHdlLLLT38"><meta name="baidu-site-verification" content="slBbq5f8WxljPytW"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.proxy.ustclug.org/css?family=Lato:300,300italic,400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/5.15.4/css/all.min.css"><link rel="stylesheet" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//unpkg.com/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//unpkg.com/pace-js@1.2.4/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"yousazoe.top",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!0,pangu:!0,comments:{style:"tabs",active:null,storage:!0,lazyload:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:3,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta property="og:type" content="article"><meta property="og:title" content="Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction"><meta property="og:url" content="https://yousazoe.top/archives/7dbe30e4.html"><meta property="og:site_name" content="Fl0w3r"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/JTkdjX.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/695q7c.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/RVCNop.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/Z5Fb1f.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/8IDsI7.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/qucLaS.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/45cNXO.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/APIhDG.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/B1oHVI.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/mOQ8Oo.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/v1tfMp.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/cvNkh1.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/ue0bnr.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/vutcBl.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/u53EUv.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/xxf0Jb.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/7t32C2.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/cpjYUT.png"><meta property="article:published_time" content="2022-07-30T12:10:10.000Z"><meta property="article:modified_time" content="2022-07-31T10:58:56.289Z"><meta property="article:author" content="Yousazoe"><meta property="article:tag" content="Computer Graphics"><meta property="article:tag" content="Ray Tracing"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/JTkdjX.png"><link rel="canonical" href="https://yousazoe.top/archives/7dbe30e4.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction | Fl0w3r</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Fl0w3r" type="application/atom+xml"></head><body itemscope="" itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Fl0w3r</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">carpe diem</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-links"><a href="/links/" rel="section"><i class="fa fa-paperclip fa-fw"></i>Links</a></li><li class="menu-item menu-item-photos"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>Photos</a></li><li class="menu-item menu-item-artitalk"><a href="/artitalk/" rel="section"><i class="fa fa-calendar fa-fw"></i>Artitalk</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-document"><a href="/docs/" rel="section"><i class="fas fa-book fa-fw"></i>Document</a></li><li class="menu-item menu-item-qexoadmin"><a href="https://blog-yousazoe-qexo.vercel.app/" rel="noopener" target="_blank"><i class="fa fa-database fa-fw"></i>QexoAdmin</a></li><li class="menu-item menu-item-gametracker"><a href="https://yousazoe.notion.site/yousazoe/b05999823bd14b57a7a6cd81fba1a1af?v=21c3398e0bdb429c9b8157b7bf12ff6a" rel="noopener" target="_blank"><i class="fas fa-trophy fa-fw"></i>GameTracker</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope="" itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://yousazoe.top/archives/7dbe30e4.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="image" content="https://img.yousazoe.top/uPic/img/blog/icon/icon.jpeg"><meta itemprop="name" content="Yousazoe"><meta itemprop="description" content="done is better than perfect"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="Fl0w3r"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2022-07-30 20:10:10" itemprop="dateCreated datePublished" datetime="2022-07-30T20:10:10+08:00">2022-07-30</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/%E5%85%89%E7%BA%BF%E8%BF%BD%E8%B8%AA-Ray-Tracing/" itemprop="url" rel="index"><span itemprop="name">光线追踪 (Ray Tracing)</span></a> </span></span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="fas fa-pen"></i></span><span>53k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span>1:37</span></span></div></header><div class="post-body" itemprop="articleBody"><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/JTkdjX.png"></p><span id="more"></span><blockquote><p>Fig. 1. In all image sets, left: 1 sample per pixel path-traced input, center: result of the proposed post-processing denoising/reconstruction pipeline, and right:<br>4096 samples per pixel reference. Leftmost highlights: the lion is barely visible in the input, but the proposed pipeline is able to produce realistic illumination<br>results without blurring the edges and high-frequency albedo details. Center highlights: the best case for the pipeline is geometry with sufficient light in the<br>input. Rightmost highlights: the worst case for the pipeline is the one with occlusions and almost no light, resulting in blurry artifacts.<br><br></p></blockquote><p>Path tracing produces realistic results including global illumination using<br>a unified simple rendering pipeline. Reducing the amount of noise to imperceptible levels without post-processing requires thousands of samples<br>per pixel (spp), while currently it is only possible to render extremely noisy<br>1 spp frames in real time with desktop GPUs. However, post-processing can<br>utilize feature buffers, which contain noise-free auxiliary data available in<br>the rendering pipeline. Previously, regression-based noise filtering methods<br>have only been used in offline rendering due to their high computational cost.<br>In this paper we propose a novel regression-based reconstruction pipeline, called Blockwise Multi-Order Feature Regression (BMFR), tailored for pathtraced 1 spp inputs that runs in real time. The high speed is achieved with a<br>fast implementation of augmented QR factorization and by using stochastic<br>regularization to address rank-deficient feature data. The proposed algorithm is 1.8× faster than the previous state-of-the-art real-time path tracing<br>reconstruction method while producing better quality frame sequences.</p><p>CCS Concepts: • <strong>Computing methodologies → Ray tracing</strong>; Rendering;<br>Image processing;</p><p><strong>Additional Key Words and Phrases</strong>: path tracing, reconstruction, regression,<br>real-time</p><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>Real-time path tracing has been a long-standing goal of graphics<br>rendering research due to its ability to produce natural soft shadows, reflections, refractions, and global illumination effects using<br>a conceptually simple unified drawing method. However, its computational complexity is a major challenge; contemporary ray tracing frameworks [AMD 2017; Parker et al. 2010; Wald et al. 2014]<br>are able to produce only around one path tracing sample per pixel<br>(spp) at real-time frame rates on desktop-class hardware. It is expected that the real-time performance will increase in the near<br>future as new generations of high-end GPUs will integrate hardware acceleration for ray tracing [Patel 2018]. Nevertheless, a linear<br>improvement in rendering quality requires a quadratic increase in<br>computational complexity: to halve the signal-to-noise ratio in path<br>tracing, the number of samples per pixel has to be quadrupled [Pharr<br>and Humphreys 2010]. Consequently, reducing the amount of noise<br>to imperceptible levels without post-processing requires thousands<br>of samples per pixel and, therefore, denoising filters are used even<br>in offline path-traced movie rendering [Goddard 2014].</p><p>The trend of rising resolutions and refresh rates, driven especially<br>by the needs of virtual reality immersion, increases the amount of<br>required computations at the same rate as the computing hardware is<br>improved. As a consequence, it is unrealistic to expect the computing<br>hardware performance to improve fast enough to support real-time<br>path tracing at high frame rates. It seems that the achievable realtime path tracing sample rates will remain around 1 spp with the<br>consumer devices of the near future [Alla Chaitanya et al. 2017;<br>Schied et al. 2017; Viitanen et al. 2018]. Therefore, there is an urgent<br>need for novel real-time post-processing denoising methods that<br>are targeted for 1 spp path-traced inputs.</p><p>Constructing high quality results from a 1 spp starting point is<br>hard even when done offline without strict real-time constraints. The<br>input has an extreme amount of noise, much more than conventional<br>image denoising algorithms can handle. However, the reconstruction<br>results can be improved by utilizing feature buffers, which contain<br>noise-free auxiliary data available from the path tracer. The buffers<br>can include useful information such as surface normals and texture<br>albedo colors. As is essential for the real-time goal, this information<br>can be extracted from a path tracer with little performance overhead.<br>Utilizing feature buffers allows reconstruction filters to, e.g., avoid<br>blurring samples across geometry edges, which is a very disturbing<br>artifact for the human eye, or it can reduce smearing the details in<br>the textures.</p><p>Moreover, fast path tracers can reproject and accumulate samples from multiple previous frames to reduce temporal noise that<br>varies between successive frames. Flickering artifacts are especially<br>noticeable by the end users. Real-time denoising algorithms must<br>specifically account for the temporal noise as there is no option of<br>simply adding more samples per pixel and the denoising needs be<br>fast enough to fit in the time slot left over from the rendering.</p><p>In this article we propose a new regression-based reconstruction<br>pipeline optimized for 1 spp input images that runs in real time on<br>desktop GPUs. The proposed method is 1.8× faster and has better<br>objective quality than the previous state-of-the-art real-time path<br>tracing reconstruction method. The article presents the following<br>contributions:</p><p>• A novel Blockwise Multi-Order Feature Regression (BMFR) algorithm, where multiple versions of the feature buffers of<br>different orders are used for fitting.</p><p>• A fast GPU-based implementation of the BMFR algorithm.</p><p>• Proposal to use stochastic regularization to address the possible rank-deficiency of the blockwise features, avoiding numerical instabilities without the extra complexity of pivoting.</p><p>In other words, the proposed algorithm combines a completely novel<br>concept (multi-order feature buffers) with a few established concepts<br>(feature regression, QR factorization). Regression-based methods<br>have typical had execution times in order of seconds [Moon et al.<br>2016] and have been considered to be applicable only in offline<br>context [Alla Chaitanya et al. 2017; Schied et al. 2017]. However,<br>we do regression in an unusual way (blockwise processing, augmented factorization with stochastic regularization) and, therefore,<br>the proposed method is the first regression-based method to achieve<br>real-time performance.</p><h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>Path tracing reconstruction methods are covered in a recent comprehensive survey article [Zwicker et al. 2015]. In general, the methods<br>can be divided into three categories based on their complexity:<br>offline methods, interactive methods, and real-time methods. Realtime methods are closest to the context of this article, but we also<br>draw ideas from and compare to methods from the other categories.</p><p>Naturally, the best reconstruction quality for path tracing can be<br>achieved with offline methods. Since there is no strict time budget,<br>offline methods can use complicated and slow algorithms. Furthermore, as they are not constrained by real-time deadlines, their execution time can vary heavily based on the input data. Typically,<br>offline methods target inputs that have more than 1 spp, because<br>it is not a problem to generate more path tracing samples if the<br>filtering itself takes a comparatively long time. In offline methods<br>it is also possible for the filtering to guide the sample generation<br>process in path tracing so that more samples are generated at problematic areas in screen space [Li et al. 2012]. Offline reconstruction<br>can be implemented, for example, with general edge-preserving<br>image filters like guided image filtering [He et al. 2013; Liu et al.<br>2017] or bilateral filtering [Tomasi and Manduchi 1998], which are<br>guided with feature buffers. Another approach is to use a neural<br>network [Kalantari et al. 2015], which can be trained even with<br>a complete set of frames from a feature-length movie [Bako et al.<br>2017]. A third approach is to fit the noise-free feature buffers to the<br>noisy image data [Bitterli et al. 2016; Moon et al. 2014, 2015].</p><p>Neural networks can also be used at interactive frame rates as<br>shown recently by Alla Chaitanya et al. [2017]. Since the quality of<br>the interactive methods is not as good as in offline methods, extra<br>care needs to be taken to address temporal stability of the results.<br>One way to address temporal noise is to use recurrent connections<br>in each neural network layer [Alla Chaitanya et al. 2017]. Sheared<br>filtering is another approach to achieve interactive frame rates [Yan<br>et al. 2015]. In contrast to the neural network approach, sheared filtering also supports effects that produce noise to the feature buffers,<br>such as motion blur [Egan et al. 2009].</p><p>Reconstruction based on the guided image filter is the closest<br>method in the literature to the proposed one which can also reach<br>interactive frame rates [Bauszat et al. 2011]. However, it is not an<br>appealing approach for real-time implementation on modern GPUs,<br>since it requires either dozens of moving window operations or generating as many summed-area tables. Moving window operations<br>involve several orders of magnitude less parallel work than a modern<br>GPU can process concurrently, whereas generating summed-area tables requires an expensive parallel scan pattern and higher precision<br>values stored in the buffers.</p><p>There is recent research interest on algorithms that can perform<br>path tracing reconstruction in real time. A way to achieve required<br>execution speed is to use approximations or variants of the bilateral<br>filter, such as a sparse bilateral filter [Mara et al. 2017], or a hierarchical filter with multiple iterations [Burt 1981] expanded with<br>customized edge-stopping functions [Dammertz et al. 2010; Schied<br>et al. 2017].</p><p>Real-time methods are typically targeted for 1 spp inputs because<br>the motivation for attempting to perform the reconstruction in real<br>time is low if the input frames must be computed offline anyway.<br>In case of 1 spp inputs and fast lower quality reconstruction, even<br>higher degree of variation is expected in the results, making temporal stability an even bigger problem with real-time methods.</p><p>Temporal stability can be improved by accumulating projected<br>frames [Yang et al. 2009], which produces a greater effective spp<br>and more static noise in world coordinate locations. A similar idea<br>can also be used for dealing with ambient occlusions [Jiménez et al.<br>2016]. However, in these reprojection-based techniques some of<br>the rendered pixels cannot utilize the accumulated data because<br>they were occluded in the previous frame. Such disocclusion events<br>can be recognized, for example, based on inconsistencies in the<br>world-space position or normal data in the feature buffers for the<br>subsequent frames. Interestingly, the reprojection method can also<br>support, for example, rigid body animations if there is a way to<br>find out where the current pixel was in the previous frame [Rosado<br>2007]. Temporal stability can be further improved, e.g., with simple<br>Temporal Anti-Aliasing (TAA) [Karis 2014], which uses colors from<br>the neighborhood of the pixel in the current frame to adjust the data<br>sampled from the previous frame. The idea of using temporal data<br>in anti-aliasing was introduced in Enhanced Subpixel Morphological<br>Antialiasing (SMAA) [Jimenez et al. 2012].</p><p>As in previous work, the proposed reconstruction algorithm also<br>utilizes TAA, and also reprojects and accumulates noisy data from<br>previous frames. However, we dynamically change the weight of the<br>new frame so that first samples after an occlusion do not get overweighted. Moreover, we add an additional step of data accumulation<br>after filtering to increase temporal stability and to avoid artifacts.<br>Moreover, instead of using the typical approximations of the bilateral<br>filter we use regression-based reconstruction, which has been previously considered too slow for real-time use cases [Alla Chaitanya<br>et al. 2017; Schied et al. 2017]. By means of applying augmented QR<br>factorization and stochastic regularization we made the regression<br>fast enough for real-time use. Finally, we introduce BMFR, where<br>multiple versions of the feature buffers of different orders are used<br>for fitting, improving the chances for the fitting to succeed.</p><h3 id="RECONSTRUCTION-PIPELINE"><a href="#RECONSTRUCTION-PIPELINE" class="headerlink" title="RECONSTRUCTION PIPELINE"></a>RECONSTRUCTION PIPELINE</h3><p>The proposed reconstruction pipeline can be divided into three<br>main phases: preprocessing, feature fitting and post-processing. The<br>phases, marked with roman numerals, are illustrated in Fig. 2 and<br>explained in subsections below. The proposed algorithm does not<br>need to guide the path tracing process in any way.</p><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>The input for the real-time reconstruction filter is a 1 spp path-traced<br>frame and its accompanying feature buffers. The 1 spp frames are<br>generated by using a rasterizer for producing the primary rays and<br>feature buffers. We use mipmapped textures in albedo. Next, we<br>do so-called next event estimation: we trace one shadow ray towards a random point in one random light source and then continue<br>path tracing by sending one secondary ray to a random direction.<br>Namely, we use multiple importance sampling [Veach and Guibas<br>1995]. The direction of the secondary ray is decided based on importance sampling. We also trace a second shadow ray from the<br>intersection point of the secondary ray. Consequently, the 1 spp<br>pixel input has one rasterized primary ray, one ray-traced secondary<br>ray and two ray-traced shadow rays. The random numbers in the<br>path tracer were generated with Wang hash [Wang et al. 2008]. The<br>ray configuration was chosen because it can be path traced in real<br>time and is able to reproduce effects like realistic global illumination,<br>soft shadows, and reflections. Every time we refer to 1 spp data in<br>this article, we refer to this ray configuration.</p><p>Before inputting the 1 spp input into our post-processing pipeline,<br>we remove first bounce surface albedo from it. Reconstructing without albedo is a common practice [Alla Chaitanya et al. 2017; Bako<br>et al. 2017; Mara et al. 2017; Schied et al. 2017] because it ensures<br>that high-frequency details in first-bounce textures are not blurred<br>by the filter. The other commonly used idea is to decompose the<br>lighting contribution to a direct and indirect component [Bauszat<br>et al. 2011; Mara et al. 2017]. However, we do not do the separation,<br>because it typically assumes that the direct lighting component is<br>noise-free. Instead, we have 1 spp path-traced soft shadows in the<br>direct component and we filter both components at once. Filtering two noisy components separately would require running the<br>pipeline twice, which does not double the runtime since heaviest<br>parts of the work can be shared. However, we did not find significant<br>quality increase and the slowdown is unacceptable in our real-time<br>context.</p><p>If the scene contains multilayer materials, the pipeline has to be<br>run separately for every material’s illumination component. However, all illumination components can be combined and filtered at<br>once if the albedo is the same for every layer. An optimization opportunity for multilayer materials is to compute a weighted sum<br>of different albedos and illuminations and filter all illuminations at<br>once [Schied et al. 2017]. Even though combining the illuminations<br>before filtering does not produce a physically correct result, this<br>approach can be used as a fast approximation.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/695q7c.png"></p><blockquote><p>Fig. 2. Overview of the proposed reconstruction pipeline. The pipeline inputs a noisy 1 spp path-traced frame and the corresponding normal and world-space<br>position buffers. It outputs a noise-free image with a good approximation of global illumination. Without the stochastic regularization, the back substitution<br>block produces NaNs and Infs due to rank deficiency.</p></blockquote><h4 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h4><p>The preprocessing phase (I) consists of temporal accumulation of<br>the noisy 1 spp data, which reprojects the previous accumulated<br>data to the new camera frame. In the reprojection process, worldspace positions and shading normals are used to test whether we<br>can accumulate previous data or have to fall back to the current<br>frame’s 1 spp path-traced result. Because of accumulation, in most<br>of the pixels the effective spp can be greater than 1 even though<br>the individual frame inputs are 1 spp. In addition, accumulation<br>improves temporal stability of the noise.</p><p>Following a previous work [Schied et al. 2017; Yang et al. 2009],<br>we compute an exponential moving average and mix 80% of the<br>history data with 20% of the current frame data. However, we apply<br>one significant modification compared to the previous work: we<br>start by computing a cumulative moving average of the samples,<br>and use the exponential moving average only after the cumulative<br>moving average weight of the new sample would be less than 20%.<br>The use of regular average on the first frames and after occlusions<br>makes sure that the first samples do not get an excessively high<br>weight, and limiting the weight to a minimum of 20% makes sure<br>that the aged data fades away.</p><p>Computing the cumulative moving average requires that we store<br>and update the sample count of every pixel. Since we are interested<br>in the sample count only if the count is small, the values can be<br>stored in just a few bits. Loading and storing, for example, 8-bit<br>integers is insignificant compared to other memory accesses of the<br>temporal accumulation.</p><p>We use bilinear sampling of the history data and do a discard test<br>for each pixel separately. The final color is normalized by the sum<br>of the accepted sample weights only, thus the discarded pixels do<br>not affect the brightness of the sample. Also the sample count data<br>is sampled using the same custom bilinear sampling and the result<br>is rounded to the closest integer value.</p><h4 id="Blockwise-Multi-Order-Feature-Regression-BMFR"><a href="#Blockwise-Multi-Order-Feature-Regression-BMFR" class="headerlink" title="Blockwise Multi-Order Feature Regression (BMFR)"></a>Blockwise Multi-Order Feature Regression (BMFR)</h4><p>The feature fitting phase (II) is based on the following feature regression operated on non-overlapping image blocks, covering the<br>entire single frame.</p><h4 id="Feature-Fitting-with-Stochastic-Regularization"><a href="#Feature-Fitting-with-Stochastic-Regularization" class="headerlink" title="Feature Fitting with Stochastic Regularization"></a>Feature Fitting with Stochastic Regularization</h4><p>We solve the least-squares problem (Equation (2)) by the Householder QR factorization (Heath 1997). Specifically, and using<br>matrix-vector notation, let us reshape the M blockwise feature<br>buffers $F_{γ_m}^{n_m}{(p,q)}$, $(p,q) \in \Omega_{i,j}$, $m = 1,…, M$, as column vectors<br>of length W , where W is the number of pixels in the block $\Omega_{i,j}$ ,<br>and let <strong>T</strong> be the W × M matrix obtained by horizontal concatenation of such column vectors. Further, let</p><p>$$<br>\widetilde{T}^{(c)} = [T, z^{(c)}]<br>$$</p><p>be the W × (M + 1) matrix obtained by augmenting <strong>T</strong> with $z^{(c)}$,<br>which is $Z^{(c)}{(p,q)}$, reshaped into a column vector of length W .<br>We expect $W \gg M$, meaning that each block has much more pixels<br>than there are feature buffers.</p><p>Assuming that $\widetilde{T}^{(c)}$ is full rank, the Householder QR factorization yields an $(M + 1) × (M + 1)$ upper triangular matrix $\widetilde{R}^{(c)}$<br>such that $\widetilde{T}^{(c)} = \widetilde{Q}^{(c)}\widetilde{R}^{(c)}$, where $\widetilde{Q}^{(c)}$<br>is a $W × (M + 1)$ matrix with orthonormal columns. Given $\widetilde{R}^{(c)}$<br>, there is no need to compute $\widetilde{Q}^{(c)}$ for solving the linear least squares problem; instead, we can solve<br>the transformed system contained in $\widetilde{R}^{(c)}$(Heath 1997, pp. 92–93).<br>By dealing just with the smaller matrix $\widetilde{R}^{(c)}$<br>we get a significant performance improvement.</p><p>Specifically, if we denote by <strong>R</strong> and by $r^{(c)}$<br>, respectively, the topleft $M × M$ sub-matrix and the top-right $M × 1$ sub-column of $\widetilde{R}^{(c)}$ ,<br>the solution $\widehat{\alpha}^{(c)}$ of Equation (2) is given as</p><p>$$<br>R \widehat{\alpha}^{(c)} = r^{(c)}<br>$$</p><p>which can be solved, for example, via back substitution, which is<br>simple and fast. Hence, $\widehat{Y}^{(c)} (p,q)$, $(p,q) \in \Omega_{i,j}$ , Equation (3) is obtained as</p><p>$$<br>\widehat{y}^{(c)} = T \widehat{\alpha}^{(c)}<br>$$</p><p>where $\widehat{y}^{(c)}$ is $\widehat{Y}^{(c)}$ reshaped into a column vector of lengthW . Observe that <strong>R</strong> (and its inverse) does not depend on $z^{(c)}$<br>, and that $r^{(c)}$ can be computed for different channels without recalculating <strong>R</strong>, which allows to process multiple channels with minimal extra<br>cost.</p><p>In practice, $\widetilde{T}^{(c)}$ may be rank-deficient, leading to numerical instabilities that break the factorization. While the rank-deficiency<br>is typically managed by pivoting, we employ stochastic regularization. That is, we add noise to the input buffers, which makes them<br>linearly independent, i.e., Equation (4) becomes</p><p>$$<br>\widetilde{T}^{(c)} = [T + N, z^{(c)}]<br>$$</p><p>where <strong>N</strong> is a W × M matrix of zero-mean independent and identically distributed noise. T within every block is scaled to be in<br>range [−1, 1], before this addition. Since the average of the noise is<br>zero, we can expect that this regularization does not increase the<br>fitting bias. The synthesis (Equation (6)) always uses the noisefree buffers T, so the noise itself is not visible in the reconstructed<br>estimate. In our implementation, we use zero-mean uniformly distributed noise over an interval [−ε,ε], thus having variance ε2/3.<br>The value of ε that worked with all our tested scenes was 0.01.<br>Much stronger noise (ε ≈ 1.0) caused visibly too bright and dark<br>constant blocks, whereas much weaker noise (ε ≈ 0.0001) failed to<br>regularize, leading to divisions by zero in the factorization.</p><h4 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h4><p>The purpose of the post-processing phase (III) is to increase temporal stability and the perceived visual quality.</p><p>First, the fitted frame is temporally accumulated, which reduces<br>blocky artifacts caused by operating the BMFR algorithm on nonoverlapping blocks and improves temporal stability. Importantly,<br>small fitting errors caused by the stochastic regularization can be<br>expected to cancel out when multiple frames are accumulated, because the injected noise has a zero mean. To aid the reduction<br>of blockiness, BMFR processes each frame over a grid of nonoverlapping blocks that is displaced with random offsets. These<br>offsets prevent the artifacts that would arise from reusing same<br>block positions on a static scene with a static camera.</p><p>This post-processing phase is essentially the same process that<br>was done in the preprocessing step to increase the effective spp.<br>However, the process is faster, because bandwidth can be saved<br>by reusing the motion vectors and discard decisions from the preprocessing phase. By loading for every pixel just two floats and<br>four Booleans, we avoid loading all five world-space positions<br>and shading normals again, all containing three channels (one<br>from current frame, and four for bilinear sampling of the previous frame).</p><p>In this second temporal accumulation we use 10% of new data<br>and 90% old data, because these values hide the block place variations. Similarly to the first temporal accumulation we use the cumulative moving average until the weight of the new sample has<br>reached the chosen 10%. Using the cumulative moving average in<br>this second temporal accumulation is crucial, since the first block<br>fitted after an occlusion is more likely to contain outlier data and<br>with the cumulative moving average it is mixed with subsequent<br>frames more quickly. For example, if we used the exponential moving average, then after three frames the weight of the very first<br>fitted data would still be more than half. With cumulative moving<br>average the weight is the same as in a regular average: one third.</p><p>As a last step of the pipeline, TAA (Karis 2014) is used. While in<br>many of the test scenes TAA decreases the quality measured by the<br>objective quality metrics, in our experience it gives more visually<br>pleasing results.</p><h3 id="COMPLEXITY-ANALYSIS"><a href="#COMPLEXITY-ANALYSIS" class="headerlink" title="COMPLEXITY ANALYSIS"></a>COMPLEXITY ANALYSIS</h3><p>Phases I and II in the proposed pipeline can be implemented using the parallel map and parallel stencil patterns. Thus, the execution time of these phases is linearly dependent on the number<br>of pixels in the input image. In these phases adding more feature<br>buffers only increases the amount of data stored in first accumulation stage. In other words, the processing can be parallelized easily,<br>because the result pixels are independent of each other. However,<br>adding more computing hardware is likely to quickly reach its limits, because all the stages are mostly memory bound.</p><p>The most important stage in the pipeline regarding the complexity analysis is the QR stage. When the number of pixels in<br>the input image is increased, the number of QR blocks grows linearly. The blocks do not affect each other in any way, so all of<br>them can be loaded and processed in parallel, and therefore performance scales linearly. In contrast, if one feature buffer is added,<br>it must be transformed by all of the previous feature buffers. The<br>transform requirement comes from the Householder reflections<br>method: the number of required transforms is $O(M(M + 1)/2) = O(M^2)$, where $M$ is the number of feature buffers. However, the<br>work per each feature buffer in the proposed method is quite small,<br>which can be seen in Figure 3. With a reasonable number of feature<br>buffers, the execution time increase is almost linear. For comparison, guided filter’s (Bauszat et al. 2011) requirement is to generate $O(M^2)$ summed-area tables. Therefore, we can include more feature buffers in the same execution time to produce results that have<br>a higher visual quality.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/RVCNop.png"></p><blockquote><p>Fig. 3. The execution time of the whole pipeline with different counts of<br>feature buffers. The QR block size used in this measurement was 32 × 32.<br>In the rest of the runtime results we use 10 feature buffers. All of the test<br>scenes have a similar runtime, since the runtime varies only in the stages<br>that access previous frame data from pixels stated by the motion vectors.</p></blockquote><h3 id="FEATURE-BUFFER-SELECTION"><a href="#FEATURE-BUFFER-SELECTION" class="headerlink" title="FEATURE BUFFER SELECTION"></a>FEATURE BUFFER SELECTION</h3><p>The choice of which feature buffers to include in the filtering is<br>crucial. Including additional feature buffers increases the computational complexity by $O(M^2)$, but the resulting quality improvement varies dramatically based on the buffer type. It is thus essential in real-time filtering to include only the most beneficial feature<br>buffers.</p><p>To this end, we measured the effects of different buffer types<br>by greedily adding all available set of buffers to find the ones that<br>helped the most. Greedy addition means that we tested every available buffer and added the one that improved the objective quality<br>the most. After each addition we started the same process again<br>with the rest of the available buffers. Figure 4 and Figure 5 show<br>the obtained results for the Sponza test scene with a static camera;<br>the corresponding results for our other test scenes yield similar<br>conclusions.</p><p>We also experimented by adding horizontal and vertical gradient buffers consisting of a horizontal or a vertical gradient from 0<br>to 1 for each block, respectively. The idea was to provide more freedom for the feature regression (2). However, the gradient buffers<br>yielded only minor quality improvements, as Figure 4 and Figure 5<br>also show. Only minor quality improvements make sense, because<br>typically there is always gradient-like data available in the world<br>position buffers.</p><p>Every channel of each feature buffer was added at once even<br>though some channels might have not contributed much to the<br>result, because otherwise the feature selection would have suffered<br>from overfitting to camera orientations.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/Z5Fb1f.png"></p><blockquote><p>Fig. 4. The effect on denoising quality as more sets of buffers are added<br>cumulatively, measured by Root Mean Square Error(RMSE) (lower is better)<br>for the Sponza test scene with a static camera. The buffers are greedily<br>added in the order specified in the legend from top to bottom.</p></blockquote><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/8IDsI7.png"></p><blockquote><p>Fig. 5. The effect on denoising quality as more sets of buffers are added<br>cumulatively, measured by Structural SIMilarity (SSIM) (Wang et al. 2004)<br>(higher is better) for the Sponza test scene with a static camera. The buffers<br>are greedily added in the order specified in the legend from top to bottom.</p></blockquote><p>Based on the aforementioned measurement, we adopted the following multi-order set of feature buffers:</p><p>$$<br>T = [1, n_x, n_y, n_z, w_x, w_y, w_z, {w_x}^2, {w_y}^2, {w_z}^2]<br>$$</p><p>where $n_x$ , $n_y$, $n_z$ are the three channels of shading normals, and<br>$w_x$ , $w_y$, $w_z$ are the three channels of world-space positions. This<br>set of buffers was selected because, as can be seen in the figures, the<br>error is decreased the most by adding the normals and the worldspace positions. The benefit of adding further buffers appears to<br>get negligible compared to increased execution time.<br>However, the computational error metrics do not reveal small<br>problematic areas in the result, and therefore, after visual examination, we decided to add the second-order world-space positions. Figure 6 illustrates the reason for this choice; world-space<br>positions are particularly useful for getting more convincing soft<br>shadows.<br>In the proposed method, the specular highlight is generated<br>from a feature buffer that happens to have data similar to the highlight. If the highlight is not well presented by the available feature<br>data, then the result improves when multiple block locations from<br>successive frames are accumulated. Adding material roughness<br>to the set of feature buffers allows illumination to vary between<br>regions inside a block, which only helps if there are materials that<br>have a roughness texture with fine details. However, the constant<br>feature buffer generates the same result if regions of the input<br>larger than block size have uniform roughness.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/qucLaS.png"></p><blockquote><p>Fig. 6. Different buffers and results with a single 64 × 64 block of BMFR. Notice how adding world-space positions squared allows the fitting to generate<br>a more realistic soft shadow under the edge. In the fast implementation we use 32 × 32 blocks, but the larger blocks visualize the benefit in a single block<br>more clearly. The results get closer to the reference when temporal accumulation averages multiple blocks from different displacements.</p></blockquote><h3 id="TEST-SETUP"><a href="#TEST-SETUP" class="headerlink" title="TEST SETUP"></a>TEST SETUP</h3><p>We measured the visual quality and execution speed of the proposed algorithm while rendering animations. To provide the algorithm with a realistic amount of accumulated frame data, which is<br>also hindered by occlusions, all except two of the test inputs had<br>continuously moving cameras. Each frame of these animations can<br>be found in the supplementary material of this article. One frame<br>consists of 1 spp input data, the corresponding feature buffers, and<br>a 4096 spp reference rendering.</p><p>In the following, we describe our test setup, which includes an<br>example implementation of the proposed algorithm and a set of<br>compared algorithms.</p><h4 id="GPU-Implementation"><a href="#GPU-Implementation" class="headerlink" title="GPU Implementation"></a>GPU Implementation</h4><p>To measure the performance of the proposed algorithm with realistic hardware, we implemented the algorithm using OpenCL<br>and optimized it for a contemporary high-end desktop GPU, AMD<br>Radeon Vega Frontier Edition. The code we wrote for the measurements is available as supplementary material of the article. The<br>primary implementation choices that affect performance as it pertains to our target hardware are described next.</p><p>The block size was chosen to be 32 × 32, because even though<br>we found that the best quality is achieved with a 64 × 64 block,<br>32 × 32 block gives us four times more parallel work to improve the<br>processing element utilization. Moreover, we need to synchronize<br>within the block, and synchronization can be done in groups of<br>256 parallel work items in the targeted hardware. Consequently,<br>already with the 32 × 32 block, the code needs to be unrolled four<br>times between the synchronization points.</p><p>For the displacement, we used a static sequence of 16 random<br>offsets, uniformly distributed over the whole set of possible offsets. The displacement is done both horizontally and vertically.<br>This number gives enough variety of displacements with the chosen blendings of history data and the new frame in temporal accumulations.</p><p>After avoiding the heavy matrix multiplications by just computing R<br>, the computation on the targeted hardware was limited by<br>the speed of accessing the data and performing reduction in local memory, i.e., computing the sum of all concurrently processed<br>elements in local memory,1 which is the fastest memory space visible to the whole block.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/45cNXO.png"></p><p>The reduction calculations are needed for the sum calculations<br>of the dot products and vector norms, both of which are computed<br>multiple times in the Householder algorithm. Reduction is also<br>used in every block to find out the local minimum and maximum of<br>every feature, which are used to scale the values to be in the same<br>range in the fitting. We implemented the reduction with parallel reduction, where all parallel processing elements process more<br>than two elements on every iteration. The number of memory accesses per iteration for different counts of elements processed at<br>once can be seen in Table 1. The fastest alternative for reduction<br>of 32 × 32 = 1,024 elements on our target hardware was experimentally determined to consist of summing 4 elements on the first<br>two iterations and 8 elements on the last two iterations. This approach appears to be a good compromise between the parallelism<br>available and the total amount of memory accesses. Fewer levels<br>per iteration gives more parallel work. In contrast, more levels per<br>iteration results in less memory accesses in total.</p><p>The largest implemented kernel was fitting, which contains almost all the stages of phase II. In contrast to what was found by<br>Laine et al. (2013), in this case a single “megakernel,” which included the heaviest stages of phase II, was the fastest, because the<br>intermediate data could be passed through fast local memory and<br>registers.</p><p>For faster data access we use half-precision floating-point numbers as the temporal storage type and order the pixels such that<br>every 32 × 32 block is at consecutive addresses in memory. Thanks<br>to the memory layout, the hardware can load and store the data<br>with faster vector accesses. It is also possible that the path tracer<br>stores the data directly in this format, because many path tracers<br>render square blocks of pixels in one work group, since then there<br>is more cache locality in the primary rays (Aila and Karras 2010).</p><h4 id="Compared-Algorithms"><a href="#Compared-Algorithms" class="headerlink" title="Compared Algorithms"></a>Compared Algorithms</h4><p>We compared the proposed algorithm to five state-of-the-art algorithms: (1) The neural network denoiser, which is freely available in the OptiX 5.0 SDK. In this article we refer to it as the<br>OptiX Neural Network Denoiser (ONND). (2) A recent state-ofthe-art real-time Monte Carlo reconstruction algorithm, Spatiotemporal Variance-Guided Filtering (SVGF) (Schied et al. 2017).<br>(3) Guided Filtering (GF) (Bauszat et al. 2011), which we consider<br>the algorithm-wise ancestor of the proposed work. (4) An off-line<br>reconstruction algorithm called Nonlinearly Weighted First-order<br>Regression (NFOR) (Bitterli et al. 2016). (5) Another real-time reconstruction method, namely An Efficient Denoising Algorithm for<br>Global Illumination (EDAGI) (Mara et al. 2017), which is separately<br>compared in Section 7.3.</p><p>The ONND implementation is based on the interactive reconstruction from the article by Alla Chaitanya et al. (2017) but differs<br>in a few ways. Most importantly, every frame is denoised individually, which causes low temporal stability. The OptiX implementation also does not separate albedo from the input before filtering.<br>Moreover, it uses a different set of feature buffers than the original<br>article. We attempted to use the filter with temporally accumulated<br>noisy data similar to our method but found that with the default<br>training set the filter is not able to discriminate between detail and<br>noisy data due to changes in noise characteristics caused by accumulation. Consequently, we had to use a 1 spp input with this denoiser. Furthermore, ONND requires that the input is tone-mapped<br>and gamma-encoded.</p><p>The authors of SVGF did not provide an implementation for accurately reproducing the results of their article. Therefore, we used<br>a freely available implementation of the algorithm in the quality<br>assessments.2 We modified the implementation to follow the original article’s algorithm more closely by running it separately for<br>direct and indirect lighting and by removing albedo before filtering. We also changed it to use the same TAA (Karis 2014) as in the<br>SVGF article.</p><p>We used our own code for the Guided Filter implementation.<br>Our implementation is based on the MATLAB code provided by<br>the authors of the original article on guided filter (He et al. 2013)<br>but has been extended to allow an arbitrary number of feature<br>buffers. As in the article by Bauszat et al. (2011), we used a fourdimensional guidance image consisting of three normal channels<br>and depth. In the article, only indirect illumination is filtered. For<br>the indirect component, we used radius 24 and epsilon 0.01 as suggested in the article. Because in our dataset also the direct illumination component is noisy, we filtered it as well with guided filter.<br>We used a smaller filter size (radius 12) to cause less blurring, and<br>therefore to improve the results. The epsilon used for direct illumination was the same as for indirect illumination. Finally, we extended the method with albedo removal and accumulation of noisy<br>data.</p><p>For NFOR we used the freely available code released by the original authors (Bitterli et al. 2016). For comparison fairness, instead<br>of using 1 spp inputs, we used the reprojected and accumulated<br>inputs and reprojected running variances, which improved the<br>quality significantly. We also applied TAA to the results, because it<br>improved subjective quality in all test scenes and objective quality<br>in approximately half of the tests scenes.</p><h3 id="RESULTS"><a href="#RESULTS" class="headerlink" title="RESULTS"></a>RESULTS</h3><p>This section reports the performance of the algorithm in terms of<br>the visual quality of the result and the execution speed with the<br>test setup described in the previous section.</p><h4 id="Objective-Quality"><a href="#Objective-Quality" class="headerlink" title="Objective Quality"></a>Objective Quality</h4><p>We used four different metrics to measure the objective quality of<br>our method compared to the other methods: Root Mean Square Error (RMSE), Structural SIMilarity (SSIM) (Wang et al. 2004), temporal error (Schied et al. 2017), and Video Multi-Method Assessment Fusion (VMAF) (Aaron et al. 2015; Li et al. 2016). The results<br>of our measurements are presented in Table 2 and Table 3, and<br>comparison images of all the methods are shown in Figure 7. The<br>known limitations of the proposed method are further discussed in<br>Section 8.</p><p>As expected, the offline comparison method NFOR is able to obtain best results in most of the scenes with most of the metrics.<br>However, the results of the proposed method are close to the NFOR<br>results with more than ten thousand times faster runtime. NFOR<br>is not originally designed for 1 spp inputs, but when we give it reprojected inputs, the effective spp count gets close to the counts<br>used in the original article.</p><p>In the majority of the test scenes, our method outperforms the<br>previous real-time methods in terms of RMSE, SSIM, and VMAF. In<br>the remaining scenes our results are still generally comparable to<br>the other real-time methods, with only marginal differences at the<br>top. In the few cases where our results are average in terms of one<br>metric, such as for RMSE in the moving light Sponza, another metric still ranks us at the top, in this case VMAF. Hence, in such cases<br>the performance difference can be at least partially attributed to inherent limitations in the simple metrics, as they disagree with each<br>other to some extent; therefore, we provide the results for several<br>metrics. Moreover, our results could be improved if only optimizing these metrics by skipping TAA in phase III, since it introduces<br>some blur in the results and thus affects RMSE, SSIM, and VMAF<br>negatively. Nevertheless, we chose to apply it due to it producing</p><p>In terms of temporal error (Schied et al. 2017), our results are<br>overall similar to those of the guided filter and ONND. SVGF yields<br>the lowest temporal error in all of the scenes, with our method being on par with it in the static scene. However, the used temporal<br>error metric is rather simple, as it only considers the average perpixel luminance differences between adjacent frames, so its correlation with subjectively perceived temporal quality variance is<br>not immediately evident. This observation is further corroborated<br>by the fact that, as Table 3 shows, the temporal error of the reference itself is typically higher than that of the reconstructed result.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/APIhDG.png"></p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/B1oHVI.png"></p><p>Hence, instead of merely focusing on the absolute error, it may<br>also be useful to consider how close the error of the reconstructed<br>result is to the error of the reference. However, similar temporal<br>error readings can be caused by completely different changes in<br>the consecutive frames. However, VMAF demonstrably correlates<br>well with subjective quality (Li et al. 2016), and in most cases our<br>method yields significantly higher VMAF results than the other<br>real-time methods.</p><h4 id="Subjective-Quality"><a href="#Subjective-Quality" class="headerlink" title="Subjective Quality"></a>Subjective Quality</h4><p>Subjective quality of the proposed method can be evaluated with<br>Figure 7. Moreover, all full resolution frames and a video are available in the supplementary material of this article.</p><p>In Figure 7 the insets of the Living room and Classroom scenes<br>represent cases where our algorithm is able to outperform the comparison methods. ONND sometimes starts to generate details that<br>are not present in the reference at all. Due to its À-Trous nature,<br>SVGF generates sometimes light artifacts that are typical to ÀTrous based methods. These are visible for example in the red inset<br>of the Living room scene. However, GF often overblurs the illumination, which might be due to poor parameter selection. We used<br>the best parameters according to the original authors.</p><p>Insets of the San Miguel scene show different foliage cases. Our<br>method produces results that are visually pleasing and believable,<br>though somewhat overblurred.</p><p>One of the main motivations of our work is visible in the red<br>insets of the Sponza scene. The proposed method can produce in<br>real time dynamic soft shadows that are very close to the reference.<br>The green insets of the same scene represent a case where there is<br>just a small amount of light and our algorithm must rely on 1 spp<br>data due to occlusions (camera is moving back and rightwards). In<br>this case the result contains some blurred artifacts.</p><p>The roughness in the Sponza (glossy) scene is 0.1 for every material. As can be seen in the red insets of the Sponza (glossy) scene in<br>Figure 7, our algorithm can perform well with even quite complex<br>specular highlights. However, the green insets of the same scene<br>represent a hard case where all of the methods fail and it is up to the<br>viewer to decide which type of imperfection is the least disturbing.<br>More discussion on the limitations of the specular highlights can<br>be found in Section 8.</p><h4 id="Comparison-to-Noise-Free-Direct-Lighting"><a href="#Comparison-to-Noise-Free-Direct-Lighting" class="headerlink" title="Comparison to Noise-Free Direct Lighting"></a>Comparison to Noise-Free Direct Lighting</h4><p>In this subsection, we report a separate comparison with EDAGI<br>(Mara et al. 2017). This method is treated separately, because it assumes a rasterized noise-free direct lighting component. Thus, it<br>is incompatible with the stochastic noisy direct lighting in our input dataset, preventing an objective comparison to the fully pathtraced reference like that in Tables 2 and 3.</p><p>Figure 8 presents some of the test scenes from the original<br>EDAGI work, as reconstructed by the proposed method from a<br>fully stochastic path-traced lighting. When comparing these images to those in their online supplementary material, it is visible<br>how realistic soft shadows produced by the stochastic direct lighting make the proposed kind of rendering compelling.</p><h4 id="Execution-Speed"><a href="#Execution-Speed" class="headerlink" title="Execution Speed"></a>Execution Speed</h4><p>The average execution times of different parts of the proposed<br>pipeline can be seen in Table 4. In the measurements we assumed<br>that the path traced 1 spp input and feature buffers are in GPU<br>buffers when we start the timer and that the result can be left to</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/mOQ8Oo.png"></p><blockquote><p>Fig. 7. Closeups highlighting the quality differences between the proposed pipeline and the comparison methods taken from animated sequences after<br>30 frames. Detailed description of the insets is in Subsection 7.2. Reference is 4,096 spp and the comparison methods are OptiX Neural Network Denoiser<br>(ONND), which is based on Alla Chaitanya et al. (2017); Spatiotemporal Variance-Guided Filtering (SVGF) (Schied et al. 2017); Guided Filtering (GF), which is<br>based on Bauszat et al. (2011); and Nonlinearly Weighted First-order Regression (NFOR) (Bitterli et al. 2016).</p></blockquote><p>another GPU buffer. That is, we model a scenario where a GPUbased path tracer has left its data to GPU buffers and at the end,<br>the results are written to the frame buffer.</p><p>All of the runtimes reported in this section are with 1, 280 × 720<br>frames. We also confirmed with measurements that, as analyzed<br>in Section 4, the runtime scales linearly relative to the number of<br>pixels.</p><p>The execution time of the proposed pipeline was stable on AMD<br>Vega Frontier Edition (variation approximately ±0.04ms) across<br>different scenes and animation frames. The only pipeline stages<br>where runtimes are affected by the input data are the ones with<br>temporal accumulation. The runtime variation is due to cache<br>misses of dispersed loads and early exits, e.g., in case of projected<br>pixels that are detected to fall outside the new frame.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/v1tfMp.png"></p><blockquote><p>Fig. 8. Some of the scenes from (Mara et al. 2017) reconstructed with the proposed work.</p></blockquote><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/cvNkh1.png"></p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/ue0bnr.png"></p><p>The proposed pipeline clearly outperforms the other algorithms<br>(listed in Table 5) in terms of execution speed. NFOR runtime is<br>left out from the table because it is in order of minutes rather<br>than milliseconds. SVGF, the previous state-of-the-art real-time<br>method, reports average execution times of 4.4ms on NVIDIA<br>Titan X (Pascal). Our 2.4ms execution time is thus 1.8× faster.<br>Moreover, SVGF’s execution time depends more on the input data,<br>because they fall back to a slower method with harder inputs. The<br>other real-time method (Mara et al. 2017) has an average execution<br>time of 9ms on NVIDIA Titan X (Pascal). However, they expect<br>noise-free direct lighting, which makes the comparison difficult.<br>Alla Chaitanya et al. (2017) report runtimes of 55ms on NVIDIA<br>Titan X (Pascal), which means the proposed pipeline is 22× faster.<br>Guided filter (Bauszat et al. 2011) execution time linearly scaled to<br>720p frame is 94ms on NVIDIA 285 GTX, and even for that number,<br>noise-free direct lighting is required. However, the article where<br>the number was reported is already a bit old and uses a previous<br>generation GPU, thus there could be room for improvement if the<br>algorithm was optimized for a modern GPU.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/vutcBl.png"></p><blockquote><p>Fig. 9. If a shadow smaller than the block size is not represented in the<br>feature buffers, then the resulting shadow can be too soft. However, bigger<br>shadows like the contact shadow of the trash can follow the reference quite<br>closely.</p></blockquote><h3 id="LIMITATIONS"><a href="#LIMITATIONS" class="headerlink" title="LIMITATIONS"></a>LIMITATIONS</h3><p>We have observed three different categories of imperfections in<br>the results of the proposed method, which we plan to address in<br>our future work:</p><p>(1) Because of the fixed sizes of the blocks, the algorithm can<br>sometimes have difficulty constructing illumination that is not<br>visible in the feature data and is smaller than the block size. Example of small soft shadows can be seen in Figure 9. Another example<br>of this is specular highlights. High values in a small area are typically blurred as can be seen in the last row of Figure 7. However,<br>different order versions of the feature buffers and block place variation reduces the problem significantly. Moreover, the quality can<br>be improved by using feature buffers containing noise-free data<br>related to the cause of the problematic illumination. The effect of<br>adding this kind of a buffer can be seen in Figure 10.</p><p>(2) The proposed algorithm is affected by the same problems as<br>the previous works that use reprojected temporal data. Since the<br>reprojection is done to the first-bounce intersection world-space<br>position, e.g., reflections and specular highlights get overblurred.<br>However, if the material is a completely reflecting mirror, then the<br>problem can be fixed by using a virtual world-space position, but<br>if there are both a reflecting and a non-reflecting component in<br>the material, we would have to store and reproject those separately (Zimmer et al. 2015). Occlusions with the reprojected data<br>also cause the input to have different amounts of effective spp<br>in different screen space areas. Different effective spp causes the<br>quality of the output of our algorithm to be decreased in the occluded areas as can be seen in Figure 11(a). The visibility of these<br>artifacts on a still frame does not correspond to their visibility on<br>a moving scene, due to how perception works. The artifacts are<br>stronger in case of fast camera movement causing larger disocclusions, but those cases are also the ones where artifacts get harder<br>to be noticed by the user’s perception (Reibman and Poole 2007).</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/u53EUv.png"></p><blockquote><p>Fig. 10. In this example, the only difference in the flat surface is its roughness. Also, albedo is constant for the whole surface but the black background makes the smoother surface seem darker. The only feature data<br>that are not constant are the two world position axes and BMFR has to<br>construct the final image from them. In Figure 10(c) we add the material<br>ID feature buffer, which allows BMFR to differentiate between the two<br>materials and, therefore, improves the results significantly.</p></blockquote><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/xxf0Jb.png"></p><blockquote><p>Fig. 11. Different artifacts from the proposed pipeline. The lack of detailed<br>texturing in the scene makes the artifacts stand out more than usual.</p></blockquote><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/7t32C2.png"></p><blockquote><p>Fig. 12. A scene with a light moving towards the camera shows the temporal lag caused by the temporal accumulation. With the proposed temporal accumulation parameters, it takes approximately 10 frames for the<br>proposed method to produce the most similar lighting.</p></blockquote><p>Reprojected temporal data also causes lag in the lighting<br>changes caused by animations. Figure 12 shows a scene with a<br>moving light. With the proposed setup (where 20% is from the<br>newest path-tracing samples and 10% is from the newest fitted<br>frame), it takes approximately 10 frames for the image to converge<br>to a similar appearance as the reference. However, in a real use case<br>where the 4096 spp reference is not available, the lag is hard to notice, since 10 frames is not a long time with the frame rates the<br>proposed algorithm is able to generate. One solution to the temporal lag problem was provided in a concurrent work (Schied et al.<br>2018). However, the same algorithm cannot be directly used with<br>the proposed work, because it would generate blocking artifacts to<br>areas where illumination changes drastically.</p><p>(3) The blockwise nature of the algorithm causes blocking artifacts visible in Figure 11(b). These can be seen on the first frame<br>when there is no accumulated data in the input and no block displacement in BMFR. On the first frame, the problem could be fixed<br>by running the fitting phase (II) twice with two different grid locations and smoothly blending the overlapping pixels from one block<br>to another. Moreover, during the first frames in a completely new<br>camera location it is hard for the human visual system to perceive<br>artifacts (Reibman and Poole 2007). However, the issue can be adequately resolved by using a fade-in effect over a few frames when<br>the camera is “teleported” to a completely new location.</p><p>We have also tested the proposed method with more than one<br>bounce of path tracing. An example of this is shown in Figure 13.<br>The only limiting factor is that the radiance of fireflies is only<br>propagated within a single block area, which is defined in screen<br>space. This limitation is not visible in typical scenes, but it can be<br>a problem in dedicated test scenes where a path to the light is very<br>unlikely to be found. However, temporal accumulation after the fitting phase robustly removes temporal artifacts caused by the fireflies. If the fireflies are very rare and there is a need for some illumination in the results, then it might be possible to use path space<br>regularization techniques (Kaplanyan and Dachsbacher 2013).</p><p>During prototyping the algorithm, we noticed that using multiple iterations of BMFR with multiple orders of features, different block locations, and different block sizes on each iteration,<br>can reduce the artifacts discussed in this section. However, having a single iteration with fixed-sized blocks was best suited for<br>our real-time implementation. Akin to multivariate monomials,<br>the extended set of feature buffers in Equation (1) may also include<br>generic products of the form $F_{n_j}^{γ_j} F_{n_k}^{γ_k}$, however this opportunity has<br>not been investigated for this work.</p><p>One more limitation of our algorithm is that noise in the feature buffers, due to, e.g., motion blur or depth of field, is visible<br>in the results. These kinds of effects would require denoising the<br>feature buffers first. However, in both examples we can compute<br>how much the data in the feature buffer should be blurred to follow the physical phenomenon. We plan to address the problem of<br>noisy feature buffers in future work.</p><h3 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h3><p>In this article, we introduced Blockwise Multi-Order Feature Regression (BMFR). In BMFR, different powers of the feature buffers are<br>used for blockwise regression in path-tracing reconstruction. We<br>show that a real time GPU-based implementation of BMFR is possible; the evaluated example implementation processes a 720p frame<br>in 2.4 ms on a modern GPU, making it 1.8× faster than the previous state-of-the-art real-time path tracing reconstruction algorithm<br>with better quality in almost all the used metrics. The code and<br>the data to reproduce our results is available in the supplementary<br>material of this article.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/cpjYUT.png"></p><blockquote><p>Fig. 13. An example of how the proposed method handles inputs that have more than one bounce. In this mostly indirect illumination case the 1-bounce<br>1 spp BMFR is slightly too dark in the green inset, since it is very unlikely that the only secondary ray finds its way out from the opening. The 9-bounce<br>1 spp BMFR is already close to the reference. However, in the red inset the fireflies on the dark wall next to the opening cause more brightness to bleed to<br>the wrong side of the corner. In these figures the gamma correction was modified so that the problems standout more clearly.</p></blockquote><p>The high execution speed of the proposed algorithm is achieved<br>by augmented QR factorization and the use of stochastic regularization, which addresses rank-deficiencies and avoids numerical instabilities without the extra complexity of pivoting. Like in previous<br>work, our algorithm relies on reprojecting and accumulating previous frames, which increases the effective samples-per-pixelcount<br>in our input. Instead of using exponential moving average for the<br>data accumulation all the time, on the first frames and after an<br>occlusion we use a cumulative moving average of the samples. Cumulative moving average does not give an excessive weight to the<br>very first samples and, therefore, reduces artifacts. In our algorithm<br>we use similar accumulation also after the regression to increase<br>the temporal stability and to decrease the amount of artifacts in the<br>results.</p><h3 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h3><ul><li><p>Anne Aaron, Zhi Li, Megha Manohara, Joe Yuchieh Lin, Eddy Chi-Hao Wu, and C.C Jay Kuo. 2015. Challenges in Cloud Based Ingest and Encoding for High Quality<br>Streaming Media. In Proceedings of the Image Processing.</p></li><li><p>Timo Aila and Tero Karras. 2010. Architecture Considerations for Tracing Incoherent<br>Rays. In Proceedings of the High Performance Graphics.</p></li><li><p>Chakravarty Alla Chaitanya, Anton Kaplanyan, Christoph Schied, Marco Salvi, Aaron<br>Lefohn, Derek Nowrouzezahrai, and Timo Aila. 2017. Interactive Reconstruction of<br>Monte Carlo Image Sequences Using a Recurrent Denoising Autoencoder. Transactions on Graphics 36, 4 (2017).</p></li><li><p>AMD. 2017. RadeonRays SDK. Online. (2017).<br>Available: <a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a><br>GPUOpen-LibrariesAndSDKs/RadeonRays_SDK, Referenced: January 23 2018.</p></li><li><p>Steve Bako, Thijs Vogels, Brian Mcwilliams, Mark Meyer, Jan NováK, Alex Harvill,<br>Pradeep Sen, Tony Derose, and Fabrice Rousselle. 2017. Kernel-predicting convolutional networks for denoising Monte Carlo renderings. Transactions on Graphics 36,<br>4 (2017).</p></li><li><p>Pablo Bauszat, Martin Eisemann, and Marcus Magnor. 2011. Guided Image Filtering<br>for Interactive High-quality Global Illumination. Computer Graphics Forum 30, 4<br>(2011).</p></li><li><p>Benedikt Bitterli. 2016.<br>Rendering resources.<br>(2016).<br><a target="_blank" rel="noopener" href="https://benediktbitterli.me/resources/">https://benediktbitterli.me/resources/</a>.</p></li><li><p>Benedikt Bitterli, Fabrice Rousselle, Bochang Moon, José A Iglesias-Guitián, David<br>Adler, Kenny Mitchell, Wojciech Jarosz, and Jan Novák. 2016. Nonlinearly Weighted<br>First-order Regression for Denoising Monte Carlo Renderings. Computer Graphics<br>Forum 35, 4 (2016).</p></li><li><p>Peter Burt. 1981. Fast Filter Transform for Image Processing. Computer Graphics and<br>Image Processing 16, 1 (1981).</p></li><li><p>Holger Dammertz, Daniel Sewtz, Johannes Hanika, and Hendrik Lensch. 2010. Edgeavoiding À-Trous Wavelet Transform for Fast Global Illumination Filtering. In<br>Proceedings of the High Performance Graphics.</p></li><li><p>Kevin Egan, Yu-Ting Tseng, Nicolas Holzschuch, Frédo Durand, and Ravi Ramamoorthi. 2009. Frequency analysis and sheared reconstruction for rendering motion blur.<br>Transactions on Graphics 28, 3 (2009), 93.</p></li><li><p>Luke Goddard. 2014. Silencing the Noise on Elysium. In ACM SIGGRAPH 2014 Talks.</p></li><li><p>Kaiming He, Jian Sun, and Xiaoou Tang. 2013. Guided Image Filtering. Transactions on<br>Pattern Analysis and Machine Intelligence 35, 6 (2013).</p></li><li><p>Michael Heath. 1997. Scientific Computing. McGraw-Hill.</p></li><li><p>Jorge Jimenez, Jose I. Echevarria, Tiago Sousa, and Diego Gutierrez. 2012. SMAA:<br>Enhanced Morphological Antialiasing. Computer Graphics Forum (Proc. EUROGRAPHICS 2012) 31, 2 (2012).</p></li><li><p>Jorge Jiménez, X Wu, A Pesce, and A Jarabo. 2016. Practical real-time strategies for<br>accurate indirect occlusion. SIGGRAPH 2016 Courses: Physically Based Shading in<br>Theory and Practice (2016).</p></li><li><p>Nima Khademi Kalantari, Steve Bako, and Pradeep Sen. 2015. A Machine Learning<br>Approach for Filtering Monte Carlo Noise. Transactions on Graphics 34, 4 (2015).</p></li><li><p>Anton Kaplanyan and Carsten Dachsbacher. 2013. Path space regularization for holistic<br>and robust light transport. Computer Graphics Forum 32, 2pt1 (2013).</p></li><li><p>Brian Karis. 2014. High-quality Temporal Supersampling. In ACM SIGGRAPH 2014,<br>Advances in Real-Time Rendering in Games.</p></li><li><p>Samuli Laine, Tero Karras, and Timo Aila. 2013. Megakernels Considered Harmful:<br>Wavefront Path Tracing on GPUs. In Proceedings of the High Performance Graphics.</p></li><li><p>Tzu-Mao Li, Yu-Ting Wu, and Yung-Yu Chuang. 2012. SURE-based Optimization for<br>Adaptive Sampling and Reconstruction. Transactions on Graphics 31, 6 (2012).</p></li><li><p>Zhi Li, Anne Aaron, Ioannis Katsavounidis, Anush Moorthy, and Megha<br>Manohara. 2016.<br>Toward a Practical Perceptual Video Quality Metric.<br>Online. (2016).<br>Available: <a target="_blank" rel="noopener" href="https://medium.com/netflix-techblog/">https://medium.com/netflix-techblog/</a><br>toward-a-practical-perceptual-video-quality-metric-653f208b9652, Referenced: January 23 2018.</p></li><li><p>Yu Liu, Changwen Zheng, Quan Zheng, and Hongliang Yuan. 2017. Removing Monte<br>Carlo Noise Using a Sobel Operator and a Guided Image Filter. The Visual Computer<br>34, 4 (2017).</p></li><li><p>Michael Mara, Morgan McGuire, Benedikt Bitterli, and Wojciech Jarosz. 2017. An<br>Efficient Denoising Algorithm for Global Illumination. In Proceedings of the High<br>Performance Graphics. <a target="_blank" rel="noopener" href="http://casual-effects.com/research/Mara2017Denoise/">http://casual-effects.com/research/Mara2017Denoise/</a></p></li><li><p>Morgan McGuire. 2017. Computer Graphics Archive. (2017).<br><a target="_blank" rel="noopener" href="https://casualeffects.com/data">https://casualeffects.com/data</a>.</p></li><li><p>Bochang Moon, Nathan Carr, and Sung-Eui Yoon. 2014. Adaptive Rendering Based on<br>Weighted Local Regression. Transactions on Graphics 33, 5 (2014).</p></li><li><p>Bochang Moon, Jose A Iglesias-Guitian, Sung-Eui Yoon, and Kenny Mitchell. 2015.<br>Adaptive Rendering with Linear Predictions. Transactions on Graphics 34, 4 (2015).</p></li><li><p>Bochang Moon, Steven McDonagh, Kenny Mitchell, and Markus Gross. 2016. Adaptive<br>Polynomial Rendering. Transactions on Graphics 35, 4 (2016).</p></li><li><p>Steven G Parker, James Bigler, Andreas Dietrich, Heiko Friedrich, Jared Hoberock,<br>David Luebke, David McAllister, Morgan McGuire, Keith Morley, Austin Robison,<br>et al. 2010. Optix: a General Purpose Ray Tracing Engine. Transactions on Graphics<br>29, 4 (2010).</p></li><li><p>ACM Transactions on Graphics, Vol. X, No. Y, Article Z. Publication date: May 2019.</p></li><li><p>Amar Patel. 2018. D3D12 Raytracing Functional Spec, v0.09. Microsoft. Available:<br><a target="_blank" rel="noopener" href="http://forums.directxtech.com/index.php?topic=5860.0">http://forums.directxtech.com/index.php?topic=5860.0</a>, Referenced: March 23 2018.</p></li><li><p>Matt Pharr and Greg Humphreys. 2010. Physically Based Rendering: From Theory to<br>Implementation (2nd ed.). Morgan Kaufmann.</p></li><li><p>Amy R Reibman and David Poole. 2007. Predicting packet-loss visibility using scene<br>characteristics. In Proceedings of the Packet Video.</p></li><li><p>Gilberto Rosado. 2007. GPU gems 3. Addison-Wesley Professional, Chapter 27. Motion<br>Blur as a Post-Processing Effect.</p></li><li><p>Christoph Schied, Anton Kaplanyan, Chris Wyman, Anjul Patney, Chakravarty R Alla<br>Chaitanya, John Burgess, Shiqiu Liu, Carsten Dachsbacher, Aaron Lefohn, and Marco<br>Salvi. 2017. Spatiotemporal Variance-guided Filtering: Real-time Reconstruction for<br>Path-traced Global Illumination. In Proceedings of the High Performance Graphics.</p></li><li><p>Christoph Schied, Christoph Peters, and Carsten Dachsbacher. 2018. Gradient Estimation for Real-Time Adaptive Temporal Filtering. Proceedings of the ACM on Computer<br>Graphics and Interactive Techniques 1, 2 (2018), 24.</p></li><li><p>Carlo Tomasi and Roberto Manduchi. 1998. Bilateral Filtering for Gray and Color<br>Images. In Proceedings of the Computer Vision.</p></li><li><p>Eric Veach and Leonidas J Guibas. 1995. Optimally combining sampling techniques<br>for Monte Carlo rendering. In Proceedings of the Computer graphics and interactive<br>techniques.</p></li><li><p>Timo Viitanen, Matias Koskela, Kalle Immonen, Markku Mäkitalo, Pekka Jääskeläinen,<br>and Jarmo Takala. 2018. Sparse Sampling for Real-time Ray Tracing. In Proceedings<br>of the GRAPP.</p></li><li><p>Ingo Wald, Sven Woop, Carsten Benthin, Gregory S. Johnson, and Manfred Ernst. 2014. Embree: A Kernel Framework for Efficient CPU Ray Tracing. Transactions on<br>Graphics 33, 4 (2014).</p></li><li><p>Yong Wang, Xiaofeng Liao, Di Xiao, and Kwok-Wo Wong. 2008. One-way hash function<br>construction based on 2D coupled map lattices. Information Sciences 178, 5 (2008).</p></li><li><p>Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image Quality<br>Assessment: from Error Visibility to Structural Similarity. Transactions on Image<br>Processing 13, 4 (2004).</p></li><li><p>Ling-Qi Yan, Soham Uday Mehta, Ravi Ramamoorthi, and Fredo Durand. 2015. Fast 4D<br>sheared filtering for interactive rendering of distribution effects. Transactions on<br>Graphics 35, 1 (2015), 7.</p></li><li><p>Lei Yang, Diego Nehab, Pedro V Sander, Pitchaya Sitthi-amorn, Jason Lawrence, and<br>Hugues Hoppe. 2009. Amortized Supersampling. Transactions on Graphics 28, 5<br>(2009).</p></li><li><p>Henning Zimmer, Fabrice Rousselle, Wenzel Jakob, Oliver Wang, David Adler, Wojciech<br>Jarosz, Olga Sorkine-Hornung, and Alexander Sorkine-Hornung. 2015. Path-space<br>Motion Estimation and Decomposition for Robust Animation Filtering. 34, 4 (2015).</p></li><li><p>Matthias Zwicker, Wojciech Jarosz, Jaakko Lehtinen, Bochang Moon, Ravi Ramamoorthi,<br>Fabrice Rousselle, Pradeep Sen, Cyril Soler, and S-E Yoon. 2015. Recent Advances<br>in Adaptive Sampling and Reconstruction for Monte Carlo Rendering. Computer<br>Graphics Forum 34, 2 (2015).</p></li></ul></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/971404c0.html" rel="bookmark">Rendering Course by Wangningbei</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8b6729fe.html" rel="bookmark">A Gentle Introduction to DirectX Raytracing 14</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/1503bc5d.html" rel="bookmark">A Gentle Introduction to DirectX Raytracing 13</a></div></li></ul><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Graphics/" rel="tag"># Computer Graphics</a> <a href="/tags/Ray-Tracing/" rel="tag"># Ray Tracing</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/b4f250e7.html" rel="prev" title="Open Yale Courses--Death"><i class="fa fa-chevron-left"></i> Open Yale Courses--Death</a></div><div class="post-nav-item"></div></div></footer></article></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:e}=CONFIG.comments;if(CONFIG.comments.storage&&(e=localStorage.getItem("comments_active")||e),e){let t=document.querySelector(`a[href="#comment-${e}"]`);t&&t.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#INTRODUCTION"><span class="nav-number">1.</span> <span class="nav-text">INTRODUCTION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RELATED-WORK"><span class="nav-number">2.</span> <span class="nav-text">RELATED WORK</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RECONSTRUCTION-PIPELINE"><span class="nav-number">3.</span> <span class="nav-text">RECONSTRUCTION PIPELINE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Input"><span class="nav-number">3.1.</span> <span class="nav-text">Input</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Preprocessing"><span class="nav-number">3.2.</span> <span class="nav-text">Preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Blockwise-Multi-Order-Feature-Regression-BMFR"><span class="nav-number">3.3.</span> <span class="nav-text">Blockwise Multi-Order Feature Regression (BMFR)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-Fitting-with-Stochastic-Regularization"><span class="nav-number">3.4.</span> <span class="nav-text">Feature Fitting with Stochastic Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Post-processing"><span class="nav-number">3.5.</span> <span class="nav-text">Post-processing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#COMPLEXITY-ANALYSIS"><span class="nav-number">4.</span> <span class="nav-text">COMPLEXITY ANALYSIS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FEATURE-BUFFER-SELECTION"><span class="nav-number">5.</span> <span class="nav-text">FEATURE BUFFER SELECTION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TEST-SETUP"><span class="nav-number">6.</span> <span class="nav-text">TEST SETUP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPU-Implementation"><span class="nav-number">6.1.</span> <span class="nav-text">GPU Implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Compared-Algorithms"><span class="nav-number">6.2.</span> <span class="nav-text">Compared Algorithms</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RESULTS"><span class="nav-number">7.</span> <span class="nav-text">RESULTS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Objective-Quality"><span class="nav-number">7.1.</span> <span class="nav-text">Objective Quality</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Subjective-Quality"><span class="nav-number">7.2.</span> <span class="nav-text">Subjective Quality</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Comparison-to-Noise-Free-Direct-Lighting"><span class="nav-number">7.3.</span> <span class="nav-text">Comparison to Noise-Free Direct Lighting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Execution-Speed"><span class="nav-number">7.4.</span> <span class="nav-text">Execution Speed</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LIMITATIONS"><span class="nav-number">8.</span> <span class="nav-text">LIMITATIONS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CONCLUSIONS"><span class="nav-number">9.</span> <span class="nav-text">CONCLUSIONS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#REFERENCES"><span class="nav-number">10.</span> <span class="nav-text">REFERENCES</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Yousazoe" src="https://img.yousazoe.top/uPic/img/blog/icon/icon.jpeg"><p class="site-author-name" itemprop="name">Yousazoe</p><div class="site-description" itemprop="description">done is better than perfect</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">281</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">44</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">70</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="mailto:zoeyousa@gmail.com" title="E-Mail → mailto:zoeyousa@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://github.com/Yousazoe" title="GitHub → https://github.com/Yousazoe" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://twitter.com/YousaZoe" title="Twitter → https://twitter.com/YousaZoe" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://www.weibo.com/6034231696/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" title="Weibo → https://www.weibo.com/6034231696/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://steamcommunity.com/profiles/76561198856466228/" title="Steam → https://steamcommunity.com/profiles/76561198856466228/" rel="noopener" target="_blank"><i class="fab fa-steam fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://www.chess.com/member/yousazoe" title="Chess → https://www.chess.com/member/yousazoe" rel="noopener" target="_blank"><i class="fa fa-chess-pawn fa-fw"></i></a> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → /atom.xml"><i class="fa fa-rss fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="twopeople"><div class="container" style="height:200px"><canvas class="illo" width="800" height="800" style="max-width:200px;max-height:200px;touch-action:none;width:640px;height:640px"></canvas></div><script src="https://img.yousazoe.top/js/twopeople1.js"></script><script src="https://img.yousazoe.top/js/zdog.dist.js"></script><script id="rendered-js" src="https://img.yousazoe.top/js/twopeople.js"></script><style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div><div class="cc-license animated" itemprop="sponsor"><link rel="preconnect" href="https://www.netlify.com"><span class="exturl cc-opacity" title="Deploy with Netlify → https://www.netlify.com" data-url="aHR0cHM6Ly93d3cubmV0bGlmeS5jb20="><img width="80" src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://www.netlify.com/img/global/badges/netlify-dark.svg" alt="Netlify"></span></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="translate-style">繁/简：<a id="translateLink" href="javascript:translatePage();">繁体</a></div><script type="text/javascript" src="/js/tw_cn.js"></script><script type="text/javascript">var defaultEncoding=2,translateDelay=0,cookieDomain="https://tding.top/",msgToTraditionalChinese="繁体",msgToSimplifiedChinese="简体",translateButtonId="translateLink";translateInitilization()</script><div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">晋ICP备2021009930号 </a><img src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/icon/beian.png" style="display:inline-block"></div><div class="copyright">© 2020 – <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Yousazoe</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="Symbols count total">4.4m</span></div><div class="powered-by"><span><a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&amp;utm_medium=referral"><img src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://cdn.jsdelivr.net/gh/YukiNoUta/cdn-static@main/blog/svg/upyun.svg" width="53" height="18" style="fill:currentColor;display:inline-block"></a></span><span class="post-meta-divider">|</span>今早雾霾蔽日，但是不要害怕，太阳依旧在云端</div><div class="busuanzi-count"><script data-pjax="" async="" src="js/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:inline"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:inline"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="//unpkg.com/animejs@3.1.0/lib/anime.min.js"></script><script src="//npm.elemecdn.com/pjax/pjax.min.js"></script><script src="//lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="//unpkg.com/lozad@1.16.0/dist/lozad.min.js"></script><script src="//lib.baomitu.com/pangu/4.0.7/pangu.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
    $(document).ready(function () {

    if(location.href.indexOf("#reloaded")==-1){
        location.href=location.href+"#reloaded";
        location.reload();
    }
}）
#在这后面可以加入程序的其他代码  


  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });

  
  NexT.boot.refresh();
  
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  
  NexT.utils.updateSidebarPosition();
});</script><script defer="" src="//img.yousazoe.top/js/three.min.js"></script><script defer="" src="//img.yousazoe.top/js/caidai.js"></script><script data-pjax="">!function(){var e,t,o,n,r=document.getElementsByTagName("link");if(0<r.length)for(i=0;i<r.length;i++)"canonical"==r[i].rel.toLowerCase()&&r[i].href&&(e=r[i].href);t=(e||window.location.protocol).split(":")[0],e=e||window.location.href,window,o=e,n=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(o)||(t="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",n?(t+="?r="+encodeURIComponent(document.referrer),o&&(t+="&l="+o)):o&&(t+="?l="+o),(new Image).src=t)}()</script><script src="/js/local-search.js"></script><script data-pjax="">document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>`${e}=${encodeURIComponent(t)}`).join("&"),r=`/lib/pdf/web/viewer.html?file=${encodeURIComponent(t)}${a}`;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script data-pjax="">document.querySelectorAll("pre.mermaid").length&&NexT.utils.getScript("//lib.baomitu.com/mermaid/8.4.8/mermaid.min.js",()=>{mermaid.initialize({theme:"forest",logLevel:3,flowchart:{curve:"linear"},gantt:{axisFormat:"%m/%d/%Y"},sequence:{actorMargin:50}})},window.mermaid)</script><div id="pjax"><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,n=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new n.options.MathItem(e.textContent,n.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},n.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script></div><script type="text/javascript" src="/js/cursor/fireworks.js"></script><script src="https://img.yousazoe.top/live2dw/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/hijiki.model.json"},display:{position:"right",width:100,height:200},mobile:{show:!0},log:!1})</script><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:5,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var e=n.imageLazyLoadSetting.isSPA,i=n.imageLazyLoadSetting.preloadRatio||1,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight*i||document.documentElement.clientHeight*i)&&function(){var e=r[a],t=e,n=function(){r=r.filter(function(t){return e!==t})},i=new Image,o=t.getAttribute("data-original");i.onload=function(){t.src=o,n()},t.src!==o&&(i.src=o)}()}o(),n.addEventListener("scroll",function(){var t=o,e=n;clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this)</script><script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>