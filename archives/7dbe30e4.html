<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico"><link rel="mask-icon" href="/images/favicon.ico" color="#222"><meta name="google-site-verification" content="35AYyqm-wpmGmXtkn-vQMrk7AkFl1Do55uHdlLLLT38"><meta name="baidu-site-verification" content="slBbq5f8WxljPytW"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.proxy.ustclug.org/css?family=Lato:300,300italic,400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/5.15.4/css/all.min.css"><link rel="stylesheet" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//unpkg.com/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//unpkg.com/pace-js@1.2.4/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"yousazoe.top",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!0,pangu:!0,comments:{style:"tabs",active:null,storage:!0,lazyload:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:3,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta property="og:type" content="article"><meta property="og:title" content="Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction"><meta property="og:url" content="https://yousazoe.top/archives/7dbe30e4.html"><meta property="og:site_name" content="Fl0w3r"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/JTkdjX.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/695q7c.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/APIhDG.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/B1oHVI.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/mOQ8Oo.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/v1tfMp.png"><meta property="og:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/cpjYUT.png"><meta property="article:published_time" content="2022-07-30T12:10:10.000Z"><meta property="article:modified_time" content="2022-07-31T02:24:20.445Z"><meta property="article:author" content="Yousazoe"><meta property="article:tag" content="Computer Graphics"><meta property="article:tag" content="Ray Tracing"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://img.yousazoe.top/uPic/img/blog/RT7/JTkdjX.png"><link rel="canonical" href="https://yousazoe.top/archives/7dbe30e4.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction | Fl0w3r</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Fl0w3r" type="application/atom+xml"></head><body itemscope="" itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Fl0w3r</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">carpe diem</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-links"><a href="/links/" rel="section"><i class="fa fa-paperclip fa-fw"></i>Links</a></li><li class="menu-item menu-item-photos"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>Photos</a></li><li class="menu-item menu-item-artitalk"><a href="/artitalk/" rel="section"><i class="fa fa-calendar fa-fw"></i>Artitalk</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-document"><a href="/docs/" rel="section"><i class="fas fa-book fa-fw"></i>Document</a></li><li class="menu-item menu-item-qexoadmin"><a href="https://blog-yousazoe-qexo.vercel.app/" rel="noopener" target="_blank"><i class="fa fa-database fa-fw"></i>QexoAdmin</a></li><li class="menu-item menu-item-gametracker"><a href="https://yousazoe.notion.site/yousazoe/b05999823bd14b57a7a6cd81fba1a1af?v=21c3398e0bdb429c9b8157b7bf12ff6a" rel="noopener" target="_blank"><i class="fas fa-trophy fa-fw"></i>GameTracker</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope="" itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://yousazoe.top/archives/7dbe30e4.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="image" content="https://img.yousazoe.top/uPic/img/blog/icon/icon.jpeg"><meta itemprop="name" content="Yousazoe"><meta itemprop="description" content="done is better than perfect"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="Fl0w3r"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2022-07-30 20:10:10" itemprop="dateCreated datePublished" datetime="2022-07-30T20:10:10+08:00">2022-07-30</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/%E5%85%89%E7%BA%BF%E8%BF%BD%E8%B8%AA-Ray-Tracing/" itemprop="url" rel="index"><span itemprop="name">光线追踪 (Ray Tracing)</span></a> </span></span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="fas fa-pen"></i></span><span>31k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span>57 mins.</span></span></div></header><div class="post-body" itemprop="articleBody"><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/JTkdjX.png"></p><span id="more"></span><blockquote><p>Fig. 1. In all image sets, left: 1 sample per pixel path-traced input, center: result of the proposed post-processing denoising/reconstruction pipeline, and right:<br>4096 samples per pixel reference. Leftmost highlights: the lion is barely visible in the input, but the proposed pipeline is able to produce realistic illumination<br>results without blurring the edges and high-frequency albedo details. Center highlights: the best case for the pipeline is geometry with sufficient light in the<br>input. Rightmost highlights: the worst case for the pipeline is the one with occlusions and almost no light, resulting in blurry artifacts.<br><br></p></blockquote><p>Path tracing produces realistic results including global illumination using<br>a unified simple rendering pipeline. Reducing the amount of noise to imperceptible levels without post-processing requires thousands of samples<br>per pixel (spp), while currently it is only possible to render extremely noisy<br>1 spp frames in real time with desktop GPUs. However, post-processing can<br>utilize feature buffers, which contain noise-free auxiliary data available in<br>the rendering pipeline. Previously, regression-based noise filtering methods<br>have only been used in offline rendering due to their high computational cost.<br>In this paper we propose a novel regression-based reconstruction pipeline, called Blockwise Multi-Order Feature Regression (BMFR), tailored for pathtraced 1 spp inputs that runs in real time. The high speed is achieved with a<br>fast implementation of augmented QR factorization and by using stochastic<br>regularization to address rank-deficient feature data. The proposed algorithm is 1.8× faster than the previous state-of-the-art real-time path tracing<br>reconstruction method while producing better quality frame sequences.</p><p>CCS Concepts: • <strong>Computing methodologies → Ray tracing</strong>; Rendering;<br>Image processing;</p><p><strong>Additional Key Words and Phrases</strong>: path tracing, reconstruction, regression,<br>real-time</p><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>Real-time path tracing has been a long-standing goal of graphics<br>rendering research due to its ability to produce natural soft shadows, reflections, refractions, and global illumination effects using<br>a conceptually simple unified drawing method. However, its computational complexity is a major challenge; contemporary ray tracing frameworks [AMD 2017; Parker et al. 2010; Wald et al. 2014]<br>are able to produce only around one path tracing sample per pixel<br>(spp) at real-time frame rates on desktop-class hardware. It is expected that the real-time performance will increase in the near<br>future as new generations of high-end GPUs will integrate hardware acceleration for ray tracing [Patel 2018]. Nevertheless, a linear<br>improvement in rendering quality requires a quadratic increase in<br>computational complexity: to halve the signal-to-noise ratio in path<br>tracing, the number of samples per pixel has to be quadrupled [Pharr<br>and Humphreys 2010]. Consequently, reducing the amount of noise<br>to imperceptible levels without post-processing requires thousands<br>of samples per pixel and, therefore, denoising filters are used even<br>in offline path-traced movie rendering [Goddard 2014].</p><p>The trend of rising resolutions and refresh rates, driven especially<br>by the needs of virtual reality immersion, increases the amount of<br>required computations at the same rate as the computing hardware is<br>improved. As a consequence, it is unrealistic to expect the computing<br>hardware performance to improve fast enough to support real-time<br>path tracing at high frame rates. It seems that the achievable realtime path tracing sample rates will remain around 1 spp with the<br>consumer devices of the near future [Alla Chaitanya et al. 2017;<br>Schied et al. 2017; Viitanen et al. 2018]. Therefore, there is an urgent<br>need for novel real-time post-processing denoising methods that<br>are targeted for 1 spp path-traced inputs.</p><p>Constructing high quality results from a 1 spp starting point is<br>hard even when done offline without strict real-time constraints. The<br>input has an extreme amount of noise, much more than conventional<br>image denoising algorithms can handle. However, the reconstruction<br>results can be improved by utilizing feature buffers, which contain<br>noise-free auxiliary data available from the path tracer. The buffers<br>can include useful information such as surface normals and texture<br>albedo colors. As is essential for the real-time goal, this information<br>can be extracted from a path tracer with little performance overhead.<br>Utilizing feature buffers allows reconstruction filters to, e.g., avoid<br>blurring samples across geometry edges, which is a very disturbing<br>artifact for the human eye, or it can reduce smearing the details in<br>the textures.</p><p>Moreover, fast path tracers can reproject and accumulate samples from multiple previous frames to reduce temporal noise that<br>varies between successive frames. Flickering artifacts are especially<br>noticeable by the end users. Real-time denoising algorithms must<br>specifically account for the temporal noise as there is no option of<br>simply adding more samples per pixel and the denoising needs be<br>fast enough to fit in the time slot left over from the rendering.</p><p>In this article we propose a new regression-based reconstruction<br>pipeline optimized for 1 spp input images that runs in real time on<br>desktop GPUs. The proposed method is 1.8× faster and has better<br>objective quality than the previous state-of-the-art real-time path<br>tracing reconstruction method. The article presents the following<br>contributions:</p><p>• A novel Blockwise Multi-Order Feature Regression (BMFR) algorithm, where multiple versions of the feature buffers of<br>different orders are used for fitting.</p><p>• A fast GPU-based implementation of the BMFR algorithm.</p><p>• Proposal to use stochastic regularization to address the possible rank-deficiency of the blockwise features, avoiding numerical instabilities without the extra complexity of pivoting.</p><p>In other words, the proposed algorithm combines a completely novel<br>concept (multi-order feature buffers) with a few established concepts<br>(feature regression, QR factorization). Regression-based methods<br>have typical had execution times in order of seconds [Moon et al.<br>2016] and have been considered to be applicable only in offline<br>context [Alla Chaitanya et al. 2017; Schied et al. 2017]. However,<br>we do regression in an unusual way (blockwise processing, augmented factorization with stochastic regularization) and, therefore,<br>the proposed method is the first regression-based method to achieve<br>real-time performance.</p><h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>Path tracing reconstruction methods are covered in a recent comprehensive survey article [Zwicker et al. 2015]. In general, the methods<br>can be divided into three categories based on their complexity:<br>offline methods, interactive methods, and real-time methods. Realtime methods are closest to the context of this article, but we also<br>draw ideas from and compare to methods from the other categories.</p><p>Naturally, the best reconstruction quality for path tracing can be<br>achieved with offline methods. Since there is no strict time budget,<br>offline methods can use complicated and slow algorithms. Furthermore, as they are not constrained by real-time deadlines, their execution time can vary heavily based on the input data. Typically,<br>offline methods target inputs that have more than 1 spp, because<br>it is not a problem to generate more path tracing samples if the<br>filtering itself takes a comparatively long time. In offline methods<br>it is also possible for the filtering to guide the sample generation<br>process in path tracing so that more samples are generated at problematic areas in screen space [Li et al. 2012]. Offline reconstruction<br>can be implemented, for example, with general edge-preserving<br>image filters like guided image filtering [He et al. 2013; Liu et al.<br>2017] or bilateral filtering [Tomasi and Manduchi 1998], which are<br>guided with feature buffers. Another approach is to use a neural<br>network [Kalantari et al. 2015], which can be trained even with<br>a complete set of frames from a feature-length movie [Bako et al.<br>2017]. A third approach is to fit the noise-free feature buffers to the<br>noisy image data [Bitterli et al. 2016; Moon et al. 2014, 2015].</p><p>Neural networks can also be used at interactive frame rates as<br>shown recently by Alla Chaitanya et al. [2017]. Since the quality of<br>the interactive methods is not as good as in offline methods, extra<br>care needs to be taken to address temporal stability of the results.<br>One way to address temporal noise is to use recurrent connections<br>in each neural network layer [Alla Chaitanya et al. 2017]. Sheared<br>filtering is another approach to achieve interactive frame rates [Yan<br>et al. 2015]. In contrast to the neural network approach, sheared filtering also supports effects that produce noise to the feature buffers,<br>such as motion blur [Egan et al. 2009].</p><p>Reconstruction based on the guided image filter is the closest<br>method in the literature to the proposed one which can also reach<br>interactive frame rates [Bauszat et al. 2011]. However, it is not an<br>appealing approach for real-time implementation on modern GPUs,<br>since it requires either dozens of moving window operations or generating as many summed-area tables. Moving window operations<br>involve several orders of magnitude less parallel work than a modern<br>GPU can process concurrently, whereas generating summed-area tables requires an expensive parallel scan pattern and higher precision<br>values stored in the buffers.</p><p>There is recent research interest on algorithms that can perform<br>path tracing reconstruction in real time. A way to achieve required<br>execution speed is to use approximations or variants of the bilateral<br>filter, such as a sparse bilateral filter [Mara et al. 2017], or a hierarchical filter with multiple iterations [Burt 1981] expanded with<br>customized edge-stopping functions [Dammertz et al. 2010; Schied<br>et al. 2017].</p><p>Real-time methods are typically targeted for 1 spp inputs because<br>the motivation for attempting to perform the reconstruction in real<br>time is low if the input frames must be computed offline anyway.<br>In case of 1 spp inputs and fast lower quality reconstruction, even<br>higher degree of variation is expected in the results, making temporal stability an even bigger problem with real-time methods.</p><p>Temporal stability can be improved by accumulating projected<br>frames [Yang et al. 2009], which produces a greater effective spp<br>and more static noise in world coordinate locations. A similar idea<br>can also be used for dealing with ambient occlusions [Jiménez et al.<br>2016]. However, in these reprojection-based techniques some of<br>the rendered pixels cannot utilize the accumulated data because<br>they were occluded in the previous frame. Such disocclusion events<br>can be recognized, for example, based on inconsistencies in the<br>world-space position or normal data in the feature buffers for the<br>subsequent frames. Interestingly, the reprojection method can also<br>support, for example, rigid body animations if there is a way to<br>find out where the current pixel was in the previous frame [Rosado<br>2007]. Temporal stability can be further improved, e.g., with simple<br>Temporal Anti-Aliasing (TAA) [Karis 2014], which uses colors from<br>the neighborhood of the pixel in the current frame to adjust the data<br>sampled from the previous frame. The idea of using temporal data<br>in anti-aliasing was introduced in Enhanced Subpixel Morphological<br>Antialiasing (SMAA) [Jimenez et al. 2012].</p><p>As in previous work, the proposed reconstruction algorithm also<br>utilizes TAA, and also reprojects and accumulates noisy data from<br>previous frames. However, we dynamically change the weight of the<br>new frame so that first samples after an occlusion do not get overweighted. Moreover, we add an additional step of data accumulation<br>after filtering to increase temporal stability and to avoid artifacts.<br>Moreover, instead of using the typical approximations of the bilateral<br>filter we use regression-based reconstruction, which has been previously considered too slow for real-time use cases [Alla Chaitanya<br>et al. 2017; Schied et al. 2017]. By means of applying augmented QR<br>factorization and stochastic regularization we made the regression<br>fast enough for real-time use. Finally, we introduce BMFR, where<br>multiple versions of the feature buffers of different orders are used<br>for fitting, improving the chances for the fitting to succeed.</p><h3 id="RECONSTRUCTION-PIPELINE"><a href="#RECONSTRUCTION-PIPELINE" class="headerlink" title="RECONSTRUCTION PIPELINE"></a>RECONSTRUCTION PIPELINE</h3><p>The proposed reconstruction pipeline can be divided into three<br>main phases: preprocessing, feature fitting and post-processing. The<br>phases, marked with roman numerals, are illustrated in Fig. 2 and<br>explained in subsections below. The proposed algorithm does not<br>need to guide the path tracing process in any way.</p><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>The input for the real-time reconstruction filter is a 1 spp path-traced<br>frame and its accompanying feature buffers. The 1 spp frames are<br>generated by using a rasterizer for producing the primary rays and<br>feature buffers. We use mipmapped textures in albedo. Next, we<br>do so-called next event estimation: we trace one shadow ray towards a random point in one random light source and then continue<br>path tracing by sending one secondary ray to a random direction.<br>Namely, we use multiple importance sampling [Veach and Guibas<br>1995]. The direction of the secondary ray is decided based on importance sampling. We also trace a second shadow ray from the<br>intersection point of the secondary ray. Consequently, the 1 spp<br>pixel input has one rasterized primary ray, one ray-traced secondary<br>ray and two ray-traced shadow rays. The random numbers in the<br>path tracer were generated with Wang hash [Wang et al. 2008]. The<br>ray configuration was chosen because it can be path traced in real<br>time and is able to reproduce effects like realistic global illumination,<br>soft shadows, and reflections. Every time we refer to 1 spp data in<br>this article, we refer to this ray configuration.</p><p>Before inputting the 1 spp input into our post-processing pipeline,<br>we remove first bounce surface albedo from it. Reconstructing without albedo is a common practice [Alla Chaitanya et al. 2017; Bako<br>et al. 2017; Mara et al. 2017; Schied et al. 2017] because it ensures<br>that high-frequency details in first-bounce textures are not blurred<br>by the filter. The other commonly used idea is to decompose the<br>lighting contribution to a direct and indirect component [Bauszat<br>et al. 2011; Mara et al. 2017]. However, we do not do the separation,<br>because it typically assumes that the direct lighting component is<br>noise-free. Instead, we have 1 spp path-traced soft shadows in the<br>direct component and we filter both components at once. Filtering two noisy components separately would require running the<br>pipeline twice, which does not double the runtime since heaviest<br>parts of the work can be shared. However, we did not find significant<br>quality increase and the slowdown is unacceptable in our real-time<br>context.</p><p>If the scene contains multilayer materials, the pipeline has to be<br>run separately for every material’s illumination component. However, all illumination components can be combined and filtered at<br>once if the albedo is the same for every layer. An optimization opportunity for multilayer materials is to compute a weighted sum<br>of different albedos and illuminations and filter all illuminations at<br>once [Schied et al. 2017]. Even though combining the illuminations<br>before filtering does not produce a physically correct result, this<br>approach can be used as a fast approximation.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/695q7c.png"></p><blockquote><p>Fig. 2. Overview of the proposed reconstruction pipeline. The pipeline inputs a noisy 1 spp path-traced frame and the corresponding normal and world-space<br>position buffers. It outputs a noise-free image with a good approximation of global illumination. Without the stochastic regularization, the back substitution<br>block produces NaNs and Infs due to rank deficiency.</p></blockquote><h4 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h4><p>The preprocessing phase (I) consists of temporal accumulation of<br>the noisy 1 spp data, which reprojects the previous accumulated<br>data to the new camera frame. In the reprojection process, worldspace positions and shading normals are used to test whether we<br>can accumulate previous data or have to fall back to the current<br>frame’s 1 spp path-traced result. Because of accumulation, in most<br>of the pixels the effective spp can be greater than 1 even though<br>the individual frame inputs are 1 spp. In addition, accumulation<br>improves temporal stability of the noise.</p><p>Following a previous work [Schied et al. 2017; Yang et al. 2009],<br>we compute an exponential moving average and mix 80% of the<br>history data with 20% of the current frame data. However, we apply<br>one significant modification compared to the previous work: we<br>start by computing a cumulative moving average of the samples,<br>and use the exponential moving average only after the cumulative<br>moving average weight of the new sample would be less than 20%.<br>The use of regular average on the first frames and after occlusions<br>makes sure that the first samples do not get an excessively high<br>weight, and limiting the weight to a minimum of 20% makes sure<br>that the aged data fades away.</p><p>Computing the cumulative moving average requires that we store<br>and update the sample count of every pixel. Since we are interested<br>in the sample count only if the count is small, the values can be<br>stored in just a few bits. Loading and storing, for example, 8-bit<br>integers is insignificant compared to other memory accesses of the<br>temporal accumulation.</p><p>We use bilinear sampling of the history data and do a discard test<br>for each pixel separately. The final color is normalized by the sum<br>of the accepted sample weights only, thus the discarded pixels do<br>not affect the brightness of the sample. Also the sample count data<br>is sampled using the same custom bilinear sampling and the result<br>is rounded to the closest integer value.</p><h4 id="Blockwise-Multi-Order-Feature-Regression-BMFR"><a href="#Blockwise-Multi-Order-Feature-Regression-BMFR" class="headerlink" title="Blockwise Multi-Order Feature Regression (BMFR)"></a>Blockwise Multi-Order Feature Regression (BMFR)</h4><p>The feature fitting phase (II) is based on the following feature regression operated on non-overlapping image blocks, covering the<br>entire single frame.</p><h3 id="COMPLEXITY-ANALYSIS"><a href="#COMPLEXITY-ANALYSIS" class="headerlink" title="COMPLEXITY ANALYSIS"></a>COMPLEXITY ANALYSIS</h3><h3 id="FEATURE-BUFFER-SELECTION"><a href="#FEATURE-BUFFER-SELECTION" class="headerlink" title="FEATURE BUFFER SELECTION"></a>FEATURE BUFFER SELECTION</h3><h3 id="TEST-SETUP"><a href="#TEST-SETUP" class="headerlink" title="TEST SETUP"></a>TEST SETUP</h3><h3 id="RESULTS"><a href="#RESULTS" class="headerlink" title="RESULTS"></a>RESULTS</h3><p>This section reports the performance of the algorithm in terms of<br>the visual quality of the result and the execution speed with the<br>test setup described in the previous section.</p><h4 id="Objective-Quality"><a href="#Objective-Quality" class="headerlink" title="Objective Quality"></a>Objective Quality</h4><p>We used four different metrics to measure the objective quality of<br>our method compared to the other methods: Root Mean Square Error (RMSE), Structural SIMilarity (SSIM) (Wang et al. 2004), temporal error (Schied et al. 2017), and Video Multi-Method Assessment Fusion (VMAF) (Aaron et al. 2015; Li et al. 2016). The results<br>of our measurements are presented in Table 2 and Table 3, and<br>comparison images of all the methods are shown in Figure 7. The<br>known limitations of the proposed method are further discussed in<br>Section 8.</p><p>As expected, the offline comparison method NFOR is able to obtain best results in most of the scenes with most of the metrics.<br>However, the results of the proposed method are close to the NFOR<br>results with more than ten thousand times faster runtime. NFOR<br>is not originally designed for 1 spp inputs, but when we give it reprojected inputs, the effective spp count gets close to the counts<br>used in the original article.</p><p>In the majority of the test scenes, our method outperforms the<br>previous real-time methods in terms of RMSE, SSIM, and VMAF. In<br>the remaining scenes our results are still generally comparable to<br>the other real-time methods, with only marginal differences at the<br>top. In the few cases where our results are average in terms of one<br>metric, such as for RMSE in the moving light Sponza, another metric still ranks us at the top, in this case VMAF. Hence, in such cases<br>the performance difference can be at least partially attributed to inherent limitations in the simple metrics, as they disagree with each<br>other to some extent; therefore, we provide the results for several<br>metrics. Moreover, our results could be improved if only optimizing these metrics by skipping TAA in phase III, since it introduces<br>some blur in the results and thus affects RMSE, SSIM, and VMAF<br>negatively. Nevertheless, we chose to apply it due to it producing</p><p>In terms of temporal error (Schied et al. 2017), our results are<br>overall similar to those of the guided filter and ONND. SVGF yields<br>the lowest temporal error in all of the scenes, with our method being on par with it in the static scene. However, the used temporal<br>error metric is rather simple, as it only considers the average perpixel luminance differences between adjacent frames, so its correlation with subjectively perceived temporal quality variance is<br>not immediately evident. This observation is further corroborated<br>by the fact that, as Table 3 shows, the temporal error of the reference itself is typically higher than that of the reconstructed result.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/APIhDG.png"></p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/B1oHVI.png"></p><p>Hence, instead of merely focusing on the absolute error, it may<br>also be useful to consider how close the error of the reconstructed<br>result is to the error of the reference. However, similar temporal<br>error readings can be caused by completely different changes in<br>the consecutive frames. However, VMAF demonstrably correlates<br>well with subjective quality (Li et al. 2016), and in most cases our<br>method yields significantly higher VMAF results than the other<br>real-time methods.</p><h4 id="Subjective-Quality"><a href="#Subjective-Quality" class="headerlink" title="Subjective Quality"></a>Subjective Quality</h4><p>Subjective quality of the proposed method can be evaluated with<br>Figure 7. Moreover, all full resolution frames and a video are available in the supplementary material of this article.</p><p>In Figure 7 the insets of the Living room and Classroom scenes<br>represent cases where our algorithm is able to outperform the comparison methods. ONND sometimes starts to generate details that<br>are not present in the reference at all. Due to its À-Trous nature,<br>SVGF generates sometimes light artifacts that are typical to ÀTrous based methods. These are visible for example in the red inset<br>of the Living room scene. However, GF often overblurs the illumination, which might be due to poor parameter selection. We used<br>the best parameters according to the original authors.</p><p>Insets of the San Miguel scene show different foliage cases. Our<br>method produces results that are visually pleasing and believable,<br>though somewhat overblurred.</p><p>One of the main motivations of our work is visible in the red<br>insets of the Sponza scene. The proposed method can produce in<br>real time dynamic soft shadows that are very close to the reference.<br>The green insets of the same scene represent a case where there is<br>just a small amount of light and our algorithm must rely on 1 spp<br>data due to occlusions (camera is moving back and rightwards). In<br>this case the result contains some blurred artifacts.</p><p>The roughness in the Sponza (glossy) scene is 0.1 for every material. As can be seen in the red insets of the Sponza (glossy) scene in<br>Figure 7, our algorithm can perform well with even quite complex<br>specular highlights. However, the green insets of the same scene<br>represent a hard case where all of the methods fail and it is up to the<br>viewer to decide which type of imperfection is the least disturbing.<br>More discussion on the limitations of the specular highlights can<br>be found in Section 8.</p><h4 id="Comparison-to-Noise-Free-Direct-Lighting"><a href="#Comparison-to-Noise-Free-Direct-Lighting" class="headerlink" title="Comparison to Noise-Free Direct Lighting"></a>Comparison to Noise-Free Direct Lighting</h4><p>In this subsection, we report a separate comparison with EDAGI<br>(Mara et al. 2017). This method is treated separately, because it assumes a rasterized noise-free direct lighting component. Thus, it<br>is incompatible with the stochastic noisy direct lighting in our input dataset, preventing an objective comparison to the fully pathtraced reference like that in Tables 2 and 3.</p><p>Figure 8 presents some of the test scenes from the original<br>EDAGI work, as reconstructed by the proposed method from a<br>fully stochastic path-traced lighting. When comparing these images to those in their online supplementary material, it is visible<br>how realistic soft shadows produced by the stochastic direct lighting make the proposed kind of rendering compelling.</p><h4 id="Execution-Speed"><a href="#Execution-Speed" class="headerlink" title="Execution Speed"></a>Execution Speed</h4><p>The average execution times of different parts of the proposed<br>pipeline can be seen in Table 4. In the measurements we assumed<br>that the path traced 1 spp input and feature buffers are in GPU<br>buffers when we start the timer and that the result can be left to</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/mOQ8Oo.png"></p><blockquote><p>Fig. 7. Closeups highlighting the quality differences between the proposed pipeline and the comparison methods taken from animated sequences after<br>30 frames. Detailed description of the insets is in Subsection 7.2. Reference is 4,096 spp and the comparison methods are OptiX Neural Network Denoiser<br>(ONND), which is based on Alla Chaitanya et al. (2017); Spatiotemporal Variance-Guided Filtering (SVGF) (Schied et al. 2017); Guided Filtering (GF), which is<br>based on Bauszat et al. (2011); and Nonlinearly Weighted First-order Regression (NFOR) (Bitterli et al. 2016).</p></blockquote><p>another GPU buffer. That is, we model a scenario where a GPUbased path tracer has left its data to GPU buffers and at the end,<br>the results are written to the frame buffer.</p><p>All of the runtimes reported in this section are with 1, 280 × 720<br>frames. We also confirmed with measurements that, as analyzed<br>in Section 4, the runtime scales linearly relative to the number of<br>pixels.</p><p>The execution time of the proposed pipeline was stable on AMD<br>Vega Frontier Edition (variation approximately ±0.04ms) across<br>different scenes and animation frames. The only pipeline stages<br>where runtimes are affected by the input data are the ones with<br>temporal accumulation. The runtime variation is due to cache<br>misses of dispersed loads and early exits, e.g., in case of projected<br>pixels that are detected to fall outside the new frame.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/v1tfMp.png"></p><blockquote><p>Fig. 8. Some of the scenes from (Mara et al. 2017) reconstructed with the proposed work.</p></blockquote><h3 id="LIMITATIONS"><a href="#LIMITATIONS" class="headerlink" title="LIMITATIONS"></a>LIMITATIONS</h3><p>We have observed three different categories of imperfections in the<br>results of the proposed method, which we plan to address in our<br>future work:</p><ol><li>Because of the fixed sizes of the blocks, the algorithm can sometimes have difficulty constructing illumination that is not visible<br>in the feature data and is smaller than the block size. Example of<br>small soft shadows can be seen in Fig. 10. Another example of this is<br>specular highlights. High values in a small area are typically blurred<br>as can be seen in the last row of Fig. 7. However, different order<br>versions of the feature buffers and block place variation reduces the<br>problem significantly. Moreover, the quality can be improved by<br>using feature buffers containing noise-free data related to the cause<br>of the problematic illumination. The effect of adding this kind of a<br>buffer can be seen in Fig. 12.</li></ol><h3 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h3><p>In this article, we introduced Blockwise Multi-Order Feature Regression (BMFR). In BMFR, different powers of the feature buffers are<br>used for blockwise regression in path-tracing reconstruction. We<br>show that a real time GPU-based implementation of BMFR is possible; the evaluated example implementation processes a 720p frame<br>in 2.4 ms on a modern GPU, making it 1.8× faster than the previous state-of-the-art real-time path tracing reconstruction algorithm<br>with better quality in almost all the used metrics. The code and<br>the data to reproduce our results is available in the supplementary<br>material of this article.</p><p><img data-src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/RT7/cpjYUT.png"></p><blockquote><p>Fig. 13. An example of how the proposed method handles inputs that have more than one bounce. In this mostly indirect illumination case the 1-bounce<br>1 spp BMFR is slightly too dark in the green inset, since it is very unlikely that the only secondary ray finds its way out from the opening. The 9-bounce<br>1 spp BMFR is already close to the reference. However, in the red inset the fireflies on the dark wall next to the opening cause more brightness to bleed to<br>the wrong side of the corner. In these figures the gamma correction was modified so that the problems standout more clearly.</p></blockquote><p>The high execution speed of the proposed algorithm is achieved<br>by augmented QR factorization and the use of stochastic regularization, which addresses rank-deficiencies and avoids numerical instabilities without the extra complexity of pivoting. Like in previous<br>work, our algorithm relies on reprojecting and accumulating previous frames, which increases the effective samples-per-pixelcount<br>in our input. Instead of using exponential moving average for the<br>data accumulation all the time, on the first frames and after an<br>occlusion we use a cumulative moving average of the samples. Cumulative moving average does not give an excessive weight to the<br>very first samples and, therefore, reduces artifacts. In our algorithm<br>we use similar accumulation also after the regression to increase<br>the temporal stability and to decrease the amount of artifacts in the<br>results.</p><h3 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h3><ul><li><p>Anne Aaron, Zhi Li, Megha Manohara, Joe Yuchieh Lin, Eddy Chi-Hao Wu, and C.C Jay Kuo. 2015. Challenges in Cloud Based Ingest and Encoding for High Quality<br>Streaming Media. In Proceedings of the Image Processing.</p></li><li><p>Timo Aila and Tero Karras. 2010. Architecture Considerations for Tracing Incoherent<br>Rays. In Proceedings of the High Performance Graphics.</p></li><li><p>Chakravarty Alla Chaitanya, Anton Kaplanyan, Christoph Schied, Marco Salvi, Aaron<br>Lefohn, Derek Nowrouzezahrai, and Timo Aila. 2017. Interactive Reconstruction of<br>Monte Carlo Image Sequences Using a Recurrent Denoising Autoencoder. Transactions on Graphics 36, 4 (2017).</p></li><li><p>AMD. 2017. RadeonRays SDK. Online. (2017).<br>Available: <a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a><br>GPUOpen-LibrariesAndSDKs/RadeonRays_SDK, Referenced: January 23 2018.</p></li><li><p>Steve Bako, Thijs Vogels, Brian Mcwilliams, Mark Meyer, Jan NováK, Alex Harvill,<br>Pradeep Sen, Tony Derose, and Fabrice Rousselle. 2017. Kernel-predicting convolutional networks for denoising Monte Carlo renderings. Transactions on Graphics 36,<br>4 (2017).</p></li><li><p>Pablo Bauszat, Martin Eisemann, and Marcus Magnor. 2011. Guided Image Filtering<br>for Interactive High-quality Global Illumination. Computer Graphics Forum 30, 4<br>(2011).</p></li><li><p>Benedikt Bitterli. 2016.<br>Rendering resources.<br>(2016).<br><a target="_blank" rel="noopener" href="https://benediktbitterli.me/resources/">https://benediktbitterli.me/resources/</a>.</p></li><li><p>Benedikt Bitterli, Fabrice Rousselle, Bochang Moon, José A Iglesias-Guitián, David<br>Adler, Kenny Mitchell, Wojciech Jarosz, and Jan Novák. 2016. Nonlinearly Weighted<br>First-order Regression for Denoising Monte Carlo Renderings. Computer Graphics<br>Forum 35, 4 (2016).</p></li><li><p>Peter Burt. 1981. Fast Filter Transform for Image Processing. Computer Graphics and<br>Image Processing 16, 1 (1981).</p></li><li><p>Holger Dammertz, Daniel Sewtz, Johannes Hanika, and Hendrik Lensch. 2010. Edgeavoiding À-Trous Wavelet Transform for Fast Global Illumination Filtering. In<br>Proceedings of the High Performance Graphics.</p></li><li><p>Kevin Egan, Yu-Ting Tseng, Nicolas Holzschuch, Frédo Durand, and Ravi Ramamoorthi. 2009. Frequency analysis and sheared reconstruction for rendering motion blur.<br>Transactions on Graphics 28, 3 (2009), 93.</p></li><li><p>Luke Goddard. 2014. Silencing the Noise on Elysium. In ACM SIGGRAPH 2014 Talks.</p></li><li><p>Kaiming He, Jian Sun, and Xiaoou Tang. 2013. Guided Image Filtering. Transactions on<br>Pattern Analysis and Machine Intelligence 35, 6 (2013).</p></li><li><p>Michael Heath. 1997. Scientific Computing. McGraw-Hill.</p></li><li><p>Jorge Jimenez, Jose I. Echevarria, Tiago Sousa, and Diego Gutierrez. 2012. SMAA:<br>Enhanced Morphological Antialiasing. Computer Graphics Forum (Proc. EUROGRAPHICS 2012) 31, 2 (2012).</p></li><li><p>Jorge Jiménez, X Wu, A Pesce, and A Jarabo. 2016. Practical real-time strategies for<br>accurate indirect occlusion. SIGGRAPH 2016 Courses: Physically Based Shading in<br>Theory and Practice (2016).</p></li><li><p>Nima Khademi Kalantari, Steve Bako, and Pradeep Sen. 2015. A Machine Learning<br>Approach for Filtering Monte Carlo Noise. Transactions on Graphics 34, 4 (2015).</p></li><li><p>Anton Kaplanyan and Carsten Dachsbacher. 2013. Path space regularization for holistic<br>and robust light transport. Computer Graphics Forum 32, 2pt1 (2013).</p></li><li><p>Brian Karis. 2014. High-quality Temporal Supersampling. In ACM SIGGRAPH 2014,<br>Advances in Real-Time Rendering in Games.</p></li><li><p>Samuli Laine, Tero Karras, and Timo Aila. 2013. Megakernels Considered Harmful:<br>Wavefront Path Tracing on GPUs. In Proceedings of the High Performance Graphics.</p></li><li><p>Tzu-Mao Li, Yu-Ting Wu, and Yung-Yu Chuang. 2012. SURE-based Optimization for<br>Adaptive Sampling and Reconstruction. Transactions on Graphics 31, 6 (2012).</p></li><li><p>Zhi Li, Anne Aaron, Ioannis Katsavounidis, Anush Moorthy, and Megha<br>Manohara. 2016.<br>Toward a Practical Perceptual Video Quality Metric.<br>Online. (2016).<br>Available: <a target="_blank" rel="noopener" href="https://medium.com/netflix-techblog/">https://medium.com/netflix-techblog/</a><br>toward-a-practical-perceptual-video-quality-metric-653f208b9652, Referenced: January 23 2018.</p></li><li><p>Yu Liu, Changwen Zheng, Quan Zheng, and Hongliang Yuan. 2017. Removing Monte<br>Carlo Noise Using a Sobel Operator and a Guided Image Filter. The Visual Computer<br>34, 4 (2017).</p></li><li><p>Michael Mara, Morgan McGuire, Benedikt Bitterli, and Wojciech Jarosz. 2017. An<br>Efficient Denoising Algorithm for Global Illumination. In Proceedings of the High<br>Performance Graphics. <a target="_blank" rel="noopener" href="http://casual-effects.com/research/Mara2017Denoise/">http://casual-effects.com/research/Mara2017Denoise/</a></p></li><li><p>Morgan McGuire. 2017. Computer Graphics Archive. (2017).<br><a target="_blank" rel="noopener" href="https://casualeffects.com/data">https://casualeffects.com/data</a>.</p></li><li><p>Bochang Moon, Nathan Carr, and Sung-Eui Yoon. 2014. Adaptive Rendering Based on<br>Weighted Local Regression. Transactions on Graphics 33, 5 (2014).</p></li><li><p>Bochang Moon, Jose A Iglesias-Guitian, Sung-Eui Yoon, and Kenny Mitchell. 2015.<br>Adaptive Rendering with Linear Predictions. Transactions on Graphics 34, 4 (2015).</p></li><li><p>Bochang Moon, Steven McDonagh, Kenny Mitchell, and Markus Gross. 2016. Adaptive<br>Polynomial Rendering. Transactions on Graphics 35, 4 (2016).</p></li><li><p>Steven G Parker, James Bigler, Andreas Dietrich, Heiko Friedrich, Jared Hoberock,<br>David Luebke, David McAllister, Morgan McGuire, Keith Morley, Austin Robison,<br>et al. 2010. Optix: a General Purpose Ray Tracing Engine. Transactions on Graphics<br>29, 4 (2010).</p></li><li><p>ACM Transactions on Graphics, Vol. X, No. Y, Article Z. Publication date: May 2019.</p></li><li><p>Amar Patel. 2018. D3D12 Raytracing Functional Spec, v0.09. Microsoft. Available:<br><a target="_blank" rel="noopener" href="http://forums.directxtech.com/index.php?topic=5860.0">http://forums.directxtech.com/index.php?topic=5860.0</a>, Referenced: March 23 2018.</p></li><li><p>Matt Pharr and Greg Humphreys. 2010. Physically Based Rendering: From Theory to<br>Implementation (2nd ed.). Morgan Kaufmann.</p></li><li><p>Amy R Reibman and David Poole. 2007. Predicting packet-loss visibility using scene<br>characteristics. In Proceedings of the Packet Video.</p></li><li><p>Gilberto Rosado. 2007. GPU gems 3. Addison-Wesley Professional, Chapter 27. Motion<br>Blur as a Post-Processing Effect.</p></li><li><p>Christoph Schied, Anton Kaplanyan, Chris Wyman, Anjul Patney, Chakravarty R Alla<br>Chaitanya, John Burgess, Shiqiu Liu, Carsten Dachsbacher, Aaron Lefohn, and Marco<br>Salvi. 2017. Spatiotemporal Variance-guided Filtering: Real-time Reconstruction for<br>Path-traced Global Illumination. In Proceedings of the High Performance Graphics.</p></li><li><p>Christoph Schied, Christoph Peters, and Carsten Dachsbacher. 2018. Gradient Estimation for Real-Time Adaptive Temporal Filtering. Proceedings of the ACM on Computer<br>Graphics and Interactive Techniques 1, 2 (2018), 24.</p></li><li><p>Carlo Tomasi and Roberto Manduchi. 1998. Bilateral Filtering for Gray and Color<br>Images. In Proceedings of the Computer Vision.</p></li><li><p>Eric Veach and Leonidas J Guibas. 1995. Optimally combining sampling techniques<br>for Monte Carlo rendering. In Proceedings of the Computer graphics and interactive<br>techniques.</p></li><li><p>Timo Viitanen, Matias Koskela, Kalle Immonen, Markku Mäkitalo, Pekka Jääskeläinen,<br>and Jarmo Takala. 2018. Sparse Sampling for Real-time Ray Tracing. In Proceedings<br>of the GRAPP.</p></li><li><p>Ingo Wald, Sven Woop, Carsten Benthin, Gregory S. Johnson, and Manfred Ernst. 2014. Embree: A Kernel Framework for Efficient CPU Ray Tracing. Transactions on<br>Graphics 33, 4 (2014).</p></li><li><p>Yong Wang, Xiaofeng Liao, Di Xiao, and Kwok-Wo Wong. 2008. One-way hash function<br>construction based on 2D coupled map lattices. Information Sciences 178, 5 (2008).</p></li><li><p>Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image Quality<br>Assessment: from Error Visibility to Structural Similarity. Transactions on Image<br>Processing 13, 4 (2004).</p></li><li><p>Ling-Qi Yan, Soham Uday Mehta, Ravi Ramamoorthi, and Fredo Durand. 2015. Fast 4D<br>sheared filtering for interactive rendering of distribution effects. Transactions on<br>Graphics 35, 1 (2015), 7.</p></li><li><p>Lei Yang, Diego Nehab, Pedro V Sander, Pitchaya Sitthi-amorn, Jason Lawrence, and<br>Hugues Hoppe. 2009. Amortized Supersampling. Transactions on Graphics 28, 5<br>(2009).</p></li><li><p>Henning Zimmer, Fabrice Rousselle, Wenzel Jakob, Oliver Wang, David Adler, Wojciech<br>Jarosz, Olga Sorkine-Hornung, and Alexander Sorkine-Hornung. 2015. Path-space<br>Motion Estimation and Decomposition for Robust Animation Filtering. 34, 4 (2015).</p></li><li><p>Matthias Zwicker, Wojciech Jarosz, Jaakko Lehtinen, Bochang Moon, Ravi Ramamoorthi,<br>Fabrice Rousselle, Pradeep Sen, Cyril Soler, and S-E Yoon. 2015. Recent Advances<br>in Adaptive Sampling and Reconstruction for Monte Carlo Rendering. Computer<br>Graphics Forum 34, 2 (2015).</p></li></ul></div><div class="popular-posts-header">Related Posts</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/971404c0.html" rel="bookmark">Rendering Course by Wangningbei</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8b6729fe.html" rel="bookmark">A Gentle Introduction to DirectX Raytracing 14</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/1503bc5d.html" rel="bookmark">A Gentle Introduction to DirectX Raytracing 13</a></div></li></ul><footer class="post-footer"><div class="post-tags"><a href="/tags/Computer-Graphics/" rel="tag"># Computer Graphics</a> <a href="/tags/Ray-Tracing/" rel="tag"># Ray Tracing</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/b4f250e7.html" rel="prev" title="Open Yale Courses--Death"><i class="fa fa-chevron-left"></i> Open Yale Courses--Death</a></div><div class="post-nav-item"></div></div></footer></article></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:e}=CONFIG.comments;if(CONFIG.comments.storage&&(e=localStorage.getItem("comments_active")||e),e){let t=document.querySelector(`a[href="#comment-${e}"]`);t&&t.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#INTRODUCTION"><span class="nav-number">1.</span> <span class="nav-text">INTRODUCTION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RELATED-WORK"><span class="nav-number">2.</span> <span class="nav-text">RELATED WORK</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RECONSTRUCTION-PIPELINE"><span class="nav-number">3.</span> <span class="nav-text">RECONSTRUCTION PIPELINE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Input"><span class="nav-number">3.1.</span> <span class="nav-text">Input</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Preprocessing"><span class="nav-number">3.2.</span> <span class="nav-text">Preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Blockwise-Multi-Order-Feature-Regression-BMFR"><span class="nav-number">3.3.</span> <span class="nav-text">Blockwise Multi-Order Feature Regression (BMFR)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#COMPLEXITY-ANALYSIS"><span class="nav-number">4.</span> <span class="nav-text">COMPLEXITY ANALYSIS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FEATURE-BUFFER-SELECTION"><span class="nav-number">5.</span> <span class="nav-text">FEATURE BUFFER SELECTION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TEST-SETUP"><span class="nav-number">6.</span> <span class="nav-text">TEST SETUP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RESULTS"><span class="nav-number">7.</span> <span class="nav-text">RESULTS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Objective-Quality"><span class="nav-number">7.1.</span> <span class="nav-text">Objective Quality</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Subjective-Quality"><span class="nav-number">7.2.</span> <span class="nav-text">Subjective Quality</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Comparison-to-Noise-Free-Direct-Lighting"><span class="nav-number">7.3.</span> <span class="nav-text">Comparison to Noise-Free Direct Lighting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Execution-Speed"><span class="nav-number">7.4.</span> <span class="nav-text">Execution Speed</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LIMITATIONS"><span class="nav-number">8.</span> <span class="nav-text">LIMITATIONS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CONCLUSIONS"><span class="nav-number">9.</span> <span class="nav-text">CONCLUSIONS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#REFERENCES"><span class="nav-number">10.</span> <span class="nav-text">REFERENCES</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Yousazoe" src="https://img.yousazoe.top/uPic/img/blog/icon/icon.jpeg"><p class="site-author-name" itemprop="name">Yousazoe</p><div class="site-description" itemprop="description">done is better than perfect</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">281</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">44</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">70</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="mailto:zoeyousa@gmail.com" title="E-Mail → mailto:zoeyousa@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://github.com/Yousazoe" title="GitHub → https://github.com/Yousazoe" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://twitter.com/YousaZoe" title="Twitter → https://twitter.com/YousaZoe" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://www.weibo.com/6034231696/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" title="Weibo → https://www.weibo.com/6034231696/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://steamcommunity.com/profiles/76561198856466228/" title="Steam → https://steamcommunity.com/profiles/76561198856466228/" rel="noopener" target="_blank"><i class="fab fa-steam fa-fw"></i></a> </span><span class="links-of-author-item"><a href="https://www.chess.com/member/yousazoe" title="Chess → https://www.chess.com/member/yousazoe" rel="noopener" target="_blank"><i class="fa fa-chess-pawn fa-fw"></i></a> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → /atom.xml"><i class="fa fa-rss fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="twopeople"><div class="container" style="height:200px"><canvas class="illo" width="800" height="800" style="max-width:200px;max-height:200px;touch-action:none;width:640px;height:640px"></canvas></div><script src="https://img.yousazoe.top/js/twopeople1.js"></script><script src="https://img.yousazoe.top/js/zdog.dist.js"></script><script id="rendered-js" src="https://img.yousazoe.top/js/twopeople.js"></script><style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div><div class="cc-license animated" itemprop="sponsor"><link rel="preconnect" href="https://www.netlify.com"><span class="exturl cc-opacity" title="Deploy with Netlify → https://www.netlify.com" data-url="aHR0cHM6Ly93d3cubmV0bGlmeS5jb20="><img width="80" src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://www.netlify.com/img/global/badges/netlify-dark.svg" alt="Netlify"></span></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="translate-style">繁/简：<a id="translateLink" href="javascript:translatePage();">繁体</a></div><script type="text/javascript" src="/js/tw_cn.js"></script><script type="text/javascript">var defaultEncoding=2,translateDelay=0,cookieDomain="https://tding.top/",msgToTraditionalChinese="繁体",msgToSimplifiedChinese="简体",translateButtonId="translateLink";translateInitilization()</script><div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">晋ICP备2021009930号 </a><img src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://img.yousazoe.top/uPic/img/blog/icon/beian.png" style="display:inline-block"></div><div class="copyright">© 2020 – <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Yousazoe</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="Symbols count total">4.4m</span></div><div class="powered-by"><span><a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&amp;utm_medium=referral"><img src="https://img.yousazoe.top/uPic/img/blog/icon/mona-loading-default.gif" data-original="https://cdn.jsdelivr.net/gh/YukiNoUta/cdn-static@main/blog/svg/upyun.svg" width="53" height="18" style="fill:currentColor;display:inline-block"></a></span><span class="post-meta-divider">|</span>今早雾霾蔽日，但是不要害怕，太阳依旧在云端</div><div class="busuanzi-count"><script data-pjax="" async="" src="js/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:inline"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:inline"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="//unpkg.com/animejs@3.1.0/lib/anime.min.js"></script><script src="//npm.elemecdn.com/pjax/pjax.min.js"></script><script src="//lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="//unpkg.com/lozad@1.16.0/dist/lozad.min.js"></script><script src="//lib.baomitu.com/pangu/4.0.7/pangu.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
    $(document).ready(function () {

    if(location.href.indexOf("#reloaded")==-1){
        location.href=location.href+"#reloaded";
        location.reload();
    }
}）
#在这后面可以加入程序的其他代码  


  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });

  
  NexT.boot.refresh();
  
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  
  NexT.utils.updateSidebarPosition();
});</script><script defer="" src="//img.yousazoe.top/js/three.min.js"></script><script defer="" src="//img.yousazoe.top/js/caidai.js"></script><script data-pjax="">!function(){var e,t,o,n,r=document.getElementsByTagName("link");if(0<r.length)for(i=0;i<r.length;i++)"canonical"==r[i].rel.toLowerCase()&&r[i].href&&(e=r[i].href);t=(e||window.location.protocol).split(":")[0],e=e||window.location.href,window,o=e,n=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(o)||(t="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",n?(t+="?r="+encodeURIComponent(document.referrer),o&&(t+="&l="+o)):o&&(t+="?l="+o),(new Image).src=t)}()</script><script src="/js/local-search.js"></script><script data-pjax="">document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>`${e}=${encodeURIComponent(t)}`).join("&"),r=`/lib/pdf/web/viewer.html?file=${encodeURIComponent(t)}${a}`;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script data-pjax="">document.querySelectorAll("pre.mermaid").length&&NexT.utils.getScript("//lib.baomitu.com/mermaid/8.4.8/mermaid.min.js",()=>{mermaid.initialize({theme:"forest",logLevel:3,flowchart:{curve:"linear"},gantt:{axisFormat:"%m/%d/%Y"},sequence:{actorMargin:50}})},window.mermaid)</script><div id="pjax"><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,n=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new n.options.MathItem(e.textContent,n.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},n.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script></div><script type="text/javascript" src="/js/cursor/fireworks.js"></script><script src="https://img.yousazoe.top/live2dw/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/hijiki.model.json"},display:{position:"right",width:100,height:200},mobile:{show:!0},log:!1})</script><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:5,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var e=n.imageLazyLoadSetting.isSPA,i=n.imageLazyLoadSetting.preloadRatio||1,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight*i||document.documentElement.clientHeight*i)&&function(){var e=r[a],t=e,n=function(){r=r.filter(function(t){return e!==t})},i=new Image,o=t.getAttribute("data-original");i.onload=function(){t.src=o,n()},t.src!==o&&(i.src=o)}()}o(),n.addEventListener("scroll",function(){var t=o,e=n;clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this)</script><script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>